[
  {
    "objectID": "about_me.html",
    "href": "about_me.html",
    "title": "Chi Zhang",
    "section": "",
    "text": "chi.zhang@medisin.uio.no\n  \n  \n    \n     andreaczhang\n  \n  \n    \n     andreasheenn@fosstodon.org\n  \n  \n    \n     Chi Zhang\n  \n\n\n\nI am a statistician, R developer and part-time lecturer at the Faculty of Medicine, University of Oslo. Over the past few years, I’ve become very interested in research software, open source and data science practices in education and public sector.\nMy interests are:\n\nReal World Data (RWD), in particular Electronic Health Records (EHR) data,\nStatistical software development in public health and clinical research,\nStatistics education with open source tools,\nQuarto.\n\nI have a MSc in Statistics from Imperial College London in 2016. Then I moved to Norway to do my PhD and some hiking. I obtained my PhD (thesis: Representation and Utilization of hospital Electronic Health Records data) at the Faculty of Medicine, University of Oslo in 2022.\nShortly after the COVID-19 pandemic started in 2020, I joined Norwegian Institute of Public Health as a data scientist (R developer + statistician) for the open source real-time analysis and public health surveillance system, Sykdomspulsen (now CSIDS). I am one of a founding members of CSIDS, the Consortium for Statistics in Disease Surveillance, where we build open source software for public health surveillance.\nSometimes I teach statistics courses (MF9130, MF9130E, ERN4110) at Oslo Centre for Biostatistics and Epidemiology (OCBE), University of Oslo. As a certified Carpentries instructor, I also teach and help with R programming at the Carpentries workshops at University of Oslo. In 2023, I lead the transition of medical statistics classroom of MF9130E into open-source (read more).\nI am an active member of Oslo User R group and Oslo R ladies group. I give talks and host workshops to learn and share.\nIn the past 11 years I have lived in the UK and Norway (7+ years). I speak English, Norwegian and Chinese, and trying to learn Italian. I like outdoor activities, and most likely I am spending my summer in the mountains."
  },
  {
    "objectID": "projects/nor_mortality/index.html",
    "href": "projects/nor_mortality/index.html",
    "title": "Mortality Surveillance in Norway",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "projects/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "href": "projects/nor_mortality/index.html#r-packages-for-mortality-surveillance",
    "title": "Mortality Surveillance in Norway",
    "section": "R packages for mortality surveillance",
    "text": "R packages for mortality surveillance\n(continue…)\n\nnowcast\nsplalert\nmortanor"
  },
  {
    "objectID": "projects/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "href": "projects/nor_mortality/index.html#collaboration-with-cause-of-death-registry",
    "title": "Mortality Surveillance in Norway",
    "section": "Collaboration with Cause of Death Registry",
    "text": "Collaboration with Cause of Death Registry"
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "Projects",
    "section": "",
    "text": "Real World Data and Public health\nMy main research interest is hospital Electronic Health Records and large health registry data. More specifically, I am currently working on quality assurance of the prescription and use of antibiotics in a hospital setting.\nIn 2020-2022, I worked at Norwegian Institute of Public Health during Covid-19 pandemic. I helped develop Sykdomspulsen (now CSIDS), a real-time infectious disease surveillance and real-time large scale reporting with open source technology.\nSince 2023, I joined NOR-Eden and Norkost projects at University of Oslo. I help develop a set of R tools to facilitate sustainable diet design.\n\n\n\n\n\n\n\n\n\n\nCSIDS\n\n\nConsortium for Statistics in Disease Surveillance\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoreden\n\n\nR tools to faciliate sustainable nutrition research\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\nOpen source data science in healthcare\nI work on open source data science projects with healthcare applications. Here are a few projects that I’m working on at the moment.\n\n\n\n\n\n\n\n\n\n\nData Apothecary’s Notes\n\n\nMy own note-taking system for a modern health data scientist\n\n\n\n\n\n\n\n\n\n\n\n\n\nMF9130E Course in R\n\n\nTransform biostatistics classroom into R and quarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nPHUSE\n\n\nContributor to the Global Healthcare Data Science Community\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/csids/index.html",
    "href": "projects/csids/index.html",
    "title": "CSIDS",
    "section": "",
    "text": "Consortium for Statistics in Disease Surveillance"
  },
  {
    "objectID": "projects/sykdomspulsen/index.html",
    "href": "projects/sykdomspulsen/index.html",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to CSIDS: the Consortium for Statistics in Disease Surveillance. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nDownload poster (Norwegian)"
  },
  {
    "objectID": "talks/ehr_20221013_ml_icu/index.html",
    "href": "talks/ehr_20221013_ml_icu/index.html",
    "title": "Machine Learning in Intensive Care Units",
    "section": "",
    "text": "This is my talk at the PhD defence day."
  },
  {
    "objectID": "talks/ph_20220616_splverse/index.html",
    "href": "talks/ph_20220616_splverse/index.html",
    "title": "Introducing Sykdomspulsen",
    "section": "",
    "text": "About the talk\nWatch the talk on YouTube\nSykdomspulsen is a real-time analysis and disease surveillance system designed at developed at the Norwegian Institute of Public Health (FHI). Sykdomspulsen processes new data collected from 15 data sources (e.g., covid-19 cases), runs 1000.000+ statistical analysis automatically for all locations (nation, county, municipality) in Norway, produces 1000+ reports and alerts for public health authorities and shares data to the public on GitHub.\nSykdomspulsen runs on a collection of R packages, the {splverse}. {splverse} is an ecosystem for infectious disease surveillance, from analysis planning, statistical analysis to reporting via visualization, shiny website and Rmarkdown generated reports. In this talk, Chi will present how Sykdomspulsen does public health real-time surveillance during the pandemic using R. Chi will introduce some of the core packages and illustrate how they work together, with an example using real surveillance data published daily on GitHub.\n\n\nAbout the speaker\nChi is currently working at the Sykdomspulsen team as a researcher and R developer, at the Norwegian Institute of Public Health. Before she joined Sykdomspulsen in the middle of the pandemic (2020), she was a PhD student at the Department of Biostatistics at University of Oslo (OCBE), working on hospital EHR data."
  },
  {
    "objectID": "talks/talks.html",
    "href": "talks/talks.html",
    "title": "Talks",
    "section": "",
    "text": "CAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations\n2024.7.8-11, Salzburg, Austria. Conference link: UseR!"
  },
  {
    "objectID": "talks/talks.html#rstats",
    "href": "talks/talks.html#rstats",
    "title": "Talks",
    "section": "Rstats",
    "text": "Rstats\n\n\n\n\n  \n\n\n\n\nBuilding Website in R: Step by Step Introduction to blogdown\n\n\n\n\n\nTalk at Oslo UseR meetup\n\n\n\n\n\n\nApr 2, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/talks.html#electronic-health-records",
    "href": "talks/talks.html#electronic-health-records",
    "title": "Talks",
    "section": "Electronic Health Records",
    "text": "Electronic Health Records\n\n\n\n\n\n\n\n\n\n\nMachine Learning in Intensive Care Units\n\n\n\n\n\nA 45 minutes trial lecture to fulfill the requirement of my PhD degree\n\n\n\n\n\nOct 13, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nNetwork Analysis of Hospital EHR data\n\n\n\n\n\nThe talk I gave at Big Insight Day 2021\n\n\n\n\n\nFeb 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nLearning from Hospital EHR data with R\n\n\n\n\n\nA 15 minutes introduction to hospital EHR data\n\n\n\n\n\nOct 28, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/ehr_20210218_biday/index.html",
    "href": "talks/ehr_20210218_biday/index.html",
    "title": "Network Analysis of Hospital EHR data",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "talks/ph_20230330_sp/index.html",
    "href": "talks/ph_20230330_sp/index.html",
    "title": "Public health surveillance and reporting",
    "section": "",
    "text": "Time and place: Mar. 30, 2023 12:00 PM–1:00 PM\nHybrid: Georg Sverdrups hus and Zoom\nEvent page"
  },
  {
    "objectID": "talks/ph_20230330_sp/index.html#about-the-topic",
    "href": "talks/ph_20230330_sp/index.html#about-the-topic",
    "title": "Public health surveillance and reporting",
    "section": "About the topic",
    "text": "About the topic\nSituational awareness is key to fast response during a public health emergency, such as COVID-19 pandemic. However, making disease surveillance reports that cover different geographical units for various metrics and data registries is both resource intensive and time consuming. Open source tools such as R packages, GitHub and Airflow can make this process automatic, reproducible and scalable.\nEvery day during the pandemic, Sykdomspulsen team at the Norwegian Institute of Public Health (FHI/NIPH) fetched data from more than 15 data sources, cleaned, censored datasets and carried out a wide range of statistical analyses. Over 1000 situational reports containing automated graphs and tables were produced before breakfast time.\nGrab you matpakke and join us for a presentation from Chi Zhang about how Sykdomspulsen team used and developed open source software to make public health surveillance and reporting more efficient, followed up by a discussion on the benefits and concerns of making these data public. We will end with an open Q&A session as usual!"
  },
  {
    "objectID": "talks/rstats_20190402_blogdown/index.html",
    "href": "talks/rstats_20190402_blogdown/index.html",
    "title": "Building Website in R: Step by Step Introduction to blogdown",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chi Zhang",
    "section": "",
    "text": "chi.zhang@medisin.uio.no\n  \n  \n    \n     andreaczhang\n  \n  \n    \n     Chi Zhang\n  \n\n  \n  \nHello! I am a statistician and R developer at the Faculty of Medicine, University of Oslo. I’m mostly interested in RWD (real-world data), such as hospital Electronic Health Record and large public health registry data.\nIn 2020-2022, I worked as part of the COVID emergency response at the Norwegian Institute of Public Health. I helped develop the open source real-time analysis and public health surveillance system, Sykdomspulsen, and co-founded now CSIDS, the Consorium of Statistics in Disease Surveillance. I’ve been giving statistical advice and collaborating with clinicians at Oslo University Hospital since 2016.\nAs a part-time lecturer, I teach statistics with R and Quarto at Oslo Centre for Biostatistics and Epidemiology (OCBE) and R programming at the Carpentries workshops at University of Oslo.\nI enjoy cooking and spending time in the mountains."
  },
  {
    "objectID": "blog/blog_20230103_blogdown2quarto/index.html",
    "href": "blog/blog_20230103_blogdown2quarto/index.html",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "",
    "text": "Since the first time I tried the “academic” template in the popular blogdown package in 2019, three years have passed. Back then, it was THE way to build a personal website using R. The “academic” template was notoriously rich in content, and my solution was to delete components, compile, if it works - great; if not, I put the deleted content back. It worked for a while.\nWhen the distill package came out (probably in 2020?), I rebooted my website since I preferred its clean, minimalistic style. The look was possibly more appropriate for websites for an organisation or tutorials rather than personal blog, yet I appreciated the simplicity.\nThen I stopped updating my website. Between mid 2020 and early 2022, I was too stressed about completing my PhD, and balancing my other two jobs wasn’t the easiest thing. During this period, my mind had been going back to the old site from time to time, but it was hard to find enough time or energy to write about stuff."
  },
  {
    "objectID": "blog/blog_20230103_blogdown2quarto/index.html#time-to-try-quarto",
    "href": "blog/blog_20230103_blogdown2quarto/index.html#time-to-try-quarto",
    "title": "Website reboot: switching from Blogdown to Quarto",
    "section": "Time to try Quarto",
    "text": "Time to try Quarto\nNow that I’ve finally completed the more pressing tasks in October 2022, I can catch up to the cool kids on twitter: create a website with Quarto!\nThere were quite a lot of discussions about Quarto in the summer 2022. I wasn’t following the discussions closely, but I remember there were quite a few talks in the Rstudio conference this year. Then more and more people switched to Quarto on Twitter. Then people I know also switched to Quarto. What’s the fuzz about?\nMy experience with Quarto is focused on websites. I have not tried other forms of publishing. So far I have created:\n\na workshop website for my colleagues\na personal website (the one you are reading right now)\nan R package (qtwAcademic)that wraps three Quarto website templates for beginners\n\nHere are a few things I like about Quarto. Given that I’m not very experienced in front-end development, these comments are going to be about ease-of-use and design, rather than the technicalities.\n\nClean look for both personal and workshop/courses\nWhen I was using “academic” template in blogdown, I liked the structure of the site: projects, talks, blog, softwares and publications sections are clearly displayed at the top. What I didn’t like is that the default homepage was a very long single page; yet its customisation wasn’t the easist. Other templates were either too simple (for blog only), or more suitable for image display (photography projects). I wanted a website that keep the good structure of “academic”, which is quite suitable for academics (hence the name); while keeping each section independent.\nWith distill I could achieve the structure I wanted; but I didn’t enjoy it too much as a personal website (at least it wasn’t as flexible as Quarto). distill is still pretty decent for organisations or documentation site.\nWith Quarto, I can achieve the desired looks for not only a personal website (with or without blogs), but also a workshop, event or even course website. This is fantastic! The top, sidebar or hybrid navigation makes the site structure very clear, especially when there are lots of content. As an aspiring lecturer at university, this is really One Quarto Rules Them All.\n\n\nFlexible yet not overwhelming\nAs I mentioned above, hacking “academic” in blogdown was not that easy - simply because there were too many folders that you are not actually supposed to modify. It was confusing to know what to change in order to achieve the desired output, and multiple folders were having the same names, making it very challenging for beginners. Ironically, this is usually the first template beginners start with!\nThat’s why I immediately fell for Quarto: you only need 4 components to make a decent minimalistic website work:\n\n_quarto.yml to control the overall layout\nindex.qmd at the root folder to control the homepage\nabout.qmd for some basic information about the creator or the website\nproject.qmd for projects or any other content that the creator wants to display\n\nThe way that _quarto.yml clearly specifies the .qmd files really helps beginners to understand where things are. This has been extremely useful for me when I wanted to learn how people made their website by reading the source code - I could understand exactly where to find the information I needed. The clear structure greatly helps the creators themselves, and also those who want to learn.\n\n\nGreat community\nRstats people have a great community. I wouldn’t be able to make my site the way I wanted if people haven’t been sharing their works. I have learned a lot by reading the source code by Dr Emi Tanaka, Dr David Schoch, Bea Milz, Prof Mine Cetinnkaya-Rundel’s STA 210 - Regression Analysis course.\nI also made my own R package that wraps three templates to create Quarto websites that are frequently used by academics, qtwAcademic. In the following days I plan to write up more detailed explanations on how to use the package, along with some new features."
  },
  {
    "objectID": "blog/blog_20230112_roche_opensource/index.html",
    "href": "blog/blog_20230112_roche_opensource/index.html",
    "title": "Open source reporting with R: clinical, public health, RSE and embrace the change",
    "section": "",
    "text": "Two days ago (Jan 11 2023) I watched a presentation by data scientists at Roche about why they are making their clinical trials in 2023 open source with R. As someone who uses R for most of the time and has done similar works (not in pharma, but in public health surveillance and reporting: watch my talk, slides to find out what we do), I watched the presentation with great interest. Here are my notes, combined with some thoughts on open-source in the industry, public sector and academia.\n\nThree reasons for why I am writing this blog\n\nNote down some of the technology which points towards the future of the field\nRelate to my experience of open-source applied in public health, specifically public health reporting\nShare some thoughts in statistical education of applied students/researchers (e.g. medicine), and training Research Software Engineers\n\n\n\nMy experience with statistical software\nTo put my opinions in perspective,\n\nI do not have experience with SAS or pharma, so I do not have first-hand knowledge on the functionality, ease-of-use or the popularity of commercial softwares in the industry.\nI did my MSc and PhD in statistics/biostatistics/medical informatics and R had always been a default choice.\nI worked in public health for a few years, where Excel is possibly the most common tool, and STATA and R are scarcely used (statisticians, epidemiologists, bioinformaticians).\nIn the past few years, my university has made the switch from SPSS to STATA for intro statistics for medical students (while students at higher level, or doing advanced analyses might use R/python), and a test-run with R might be in motion.\n\n\n\n\nClinical Reporting\nIn drug development at pharmaceutical companies (and/or research institutes and hospitals), these data related tasks are very common:\n\nsummarise safety and efficacy data\nprovide accurate picture of trial outcomes\nmanage data collection across different sites\n\nCompleting these tasks in a correct, efficient and reproducible manner is crucial for patient safety. However, these tasks are also highly resource intensive: highly trained scientist, statisticians and technincians must be involved in the process. Historically, pharma use commercial software such as SAS.\n\nRegulation and exploration needs\nThere are requirements for clinical reporting: both regulartory and exploratory. From the regulatory side, there exist industry standards (CDISC) in the clinical research process, such as SDTM (Model for Tabulation of Study Data) and ADaM (Analysis Data Model). Statistical analyses, tables, listings and graphs (TLGs) also fall into this cateogory.\nFrom the exploratory side, clinical data are highly context dependent, and new formats of data such as imaging are more and more used in prediction modeling and drug development.\nIn addition, it is not hard to imagine that the technical competency of employees differ, especially in large organizations. Enabling people with less experience to analyse trial data in a reproducible manner is helpful for not only the learning and growth of employees, but also the productivity of organizations.\nThe existing commercial tools are not able to adapt to the rapid changes in the field.\n\n\nTransition into Open-Source\nIn this talk, Dr Kieran Martin at Roche introduced that they started using R as their core data science tool, aiming to move their codebase to having a core R. In the future, they plan to have something that is lanugage agnostic: meaning that python, Stan, C++, Julia and beyond can be used for different tasks.\nI only noted down a few of the things they mentioned on the infrastructure side:\n\nOCEAN - a lanugage agnostic computing platform on AWS (docker)\nGit, Gitlab for version control and collaboration\nRstudio connect server\nSnakemake for orchestrate production\n\n\n\nR and Shiny\nThere are obvious benefits of using R. It is convenient to install and use (if you used python and R, you’d probably agree), and the latest development in Shiny made it very easy to develop interactive visualizations, suitable for exploration. Package development is critical for reproducibility and distributing works - which R does it very well. A few packages developed by pharma are Teal and admiral: the ADaM in R, which I intend to check out at one point.\nR has deep roots in academia which means the newest statistical methods are well covered; which also affects the skill sets that talents own - fresh graduates probably already learned it at university. R being open source means that collaboration with external partners is much more efficient, and transparent. Strong community support is another positive thing that encourages beginners to enter the field and learn.\n\n\n\n\nOpen Sourcing Public Health\n\nSurveillance and reporting\nOne key functionality of public health (PH) authorities is stay informed and inform. They collect data from labs, hospitals and clinics across the country, summarize into useful statistics in tables and graphs, make reports, then inform the policy makers to make decisions (such as vaccination campaigns).\nCompared to clinical reporting (in my understanding), there are many similarities - we make TLG (tables, listings and graphs). There are also features that make reporting in public health unique:\n\nPH surveillance and reporting are dynamic and real-time, which can change in a matter of days. That is because the situation of different infectious diseases can evolve rapidly, so PH authorities need to make appropriate adjustments.\nTime and location (spatial-temporal) are important. Different time granularity (daily, weekly) and geographical units (nation, county, municipality, city districts) are typically required for reporting.\n\n\n\nScale up and automate with open source tools\nTraditionally, these reports are made manually - one location, one graph per time on a certain disease. When a global pandemic hits, this is definitely not fast enough. At my team (Sykdomspulsen team at the Norwegian Institute of Public Health), we tried a different approach. Details of what we did can be found in this talk(slides), but to make it brief:\n\nWe developed a fully automated pipeline that connects 15 registries (vaccination, lab, hospital and intensive care and many more). The data is gathered, censored, cleaned and pre-processed for down-stream analysis\nStatistical analysis, tables, graphs and maps are made for all locations in Norway for various outcomes of interest, such as Covid, influenza, respiratory and gastrointestinal infections\nOver 1000 customized reports with over 30 graphs and tables are produced daily and sent to local PH officials, where we also had a shiny website (Kommunehelsetjenester for Kommunelege) for over 300 PH officials to get most up-to-date information about their own municipality\n\nBy automation, every year Sykdomspulsen can save 700 000 NOK (roughly 70k USD) while making 400 times more real-time reports for public health. Even better, with reproducibility and quality control.\n\n\nToolbox\nSykdomspulsen is a small team (8 people, 3 are statisticians and 1 engineer), and our infrastructure was built upon R packages, which we call splverse. Our infrastructure is not fundamentally different from the one Roche introduced, basically:\n\nR does the task planning and project organization. On top of this, the data cleaning, statistical analysis are implemented. Graphs, tables and maps are made with appropriate R packages\nRmarkdown does automated reporting into .docx and .xlsx. Some reports are also in .html tables to be embedded into customized emails\nRstudio Workbench and GitHub help with teamwork\n\nDocker, GoCD and Airflow do the CI/CD and orchestration\n\n\n\n\nEmbrace the transition\n\nCulture change needed\nUnfortunately, not all organizations are eager to abandon the old way. Even at our own institute where researchers are the majority, open source and modern day programming is hardly practiced (by my observation). Even worse, under the budget cuts in 2023-24, a large number of younger employees who have the technical skills have left - which left the public health surveillance even more vulnerable now that Covid is far from over.\nIn my opinion, public health needs open-source and good programming even more than pharmaceutical companies. Both save lifes - and PH has less money to invest in softwares, infrastructures and talents. In this situation, resources should be spent in fields that are critical and most cost-effective; yet in reality this is often not the case.\nThe slow culture change at big organizations can happen, but only if there is a sufficient amount of employees who are willing to embrace the new technology. In the talk by Roche they about about their training strategy. It is not possible to train all users, and not everyone has the same needs at the same time. Therefore, self study with certain study paths is encouraged and supported.\n\n\nTeach programming to students in various fields\nBased on my experience in the UK and Norway, students (myself included) learn R programming in one of the two ways\n\nLearning by Googling (self-taught): a university degree needs to use it: provides a short introduction, then students learn by using. This is how I learned R at my MSc Statistics degree, and this is probably the most common way\nWorkshops at university: organizations such as the Carpentries provide course material and teaching a few times per year, where interested students (usually from subjects such as biology) come and learn. These classes are quite popular, and usually have a long waiting list.\n\nFrom learning by googling to some organized teaching - that is already some good progress. However, if not, can we improve?\nIn my experience with statistical advising with the university hospital, clinical researchers and medical students are enthusastic to get their statistics done, some are also eager to do some analysis themselves. That is good. Yet, there is generally lack of capacity - either knowledge or software skills. Once the statistician who helps with the project stops, the project ends. There is the need to have in-house statistical capacity. To this end, open-source softwares such as R, and good programming practice (reproducibility for example) can help a lot: the license doesn’t end, and everything is documented so that the next person can continue the work.\nI’m glad that my university has made some transitional efforts in this regard: STATA instead of SPSS is being taught to medical students as part of their statistics course. There might be a test-run in R soon, which is very exciting (since I’ll be involved in the teaching)!\n\n\nStatistical engineering and RSEs\nThat was the capacity building to get beginners more independent. On the other side, there is also the need for better programming practice for researchers at more advanced level. Research Software Engineering (RSE) is starting to get more and more attention, because it is not only relevant for research (i.e. getting papers published), but in broader applications.\nFor example, in the talk by Roche, they mentioned that “RSE teams need to accelerate adoption of new statistical methods and biomarker data analysis”, and the implementation with R packages and templates is at its core. In the future more languages would be included such as Python, Stan, C++ and Julia.\nHowever, RSE as a job title or career path is still a new thing. I know two RSEs at my university, and RSE is definitely not your typical academic faculty position: only departments that think it’s important makes positions, often not permanent. To get any new methods actually used in either industry or the public sector outside research, translating methods into tools is must-do. In the future I hope RSE becomes a stable and common career path, and more exciting things can happen."
  },
  {
    "objectID": "blog/readnotes_2023010x_preventable_sridhar/index.html",
    "href": "blog/readnotes_2023010x_preventable_sridhar/index.html",
    "title": "Preventable: How a Pandemic Changed the World & How to Stop the Next One - Devi Sridhar",
    "section": "",
    "text": "Advice on some measures to prepare for the next pandemic\n(From Five ways to prepare for the next pandemic by Prof. Devi Sridhar)\n\nMonitor zoonoses. Identify patogens with pandemic potential, regulate better wet markets\nSequence globally. Investment in genetic-sequencing capability\nStrengthen manufacturing. Vaccine inequality, fragility of vaccine production. Private and public sector work together - vaccine research, production and distribution.\nVaccine preparedness. For known diseases (e.g. influenza), invest in vaccines that protect against a wide range of variants. New technology and research for unknown threats\nStop the spread (long enough for the vaccines) to save lives."
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "",
    "text": "Some mistakes\n\nFederal agencies refused to share data\nPersons responsible did not have training inn epidemiology\nNot enough testing, not fast enough\nLacks information sharing\n\n\n\n\nThere should be a global expert team to help preventing the pandemic: they should be responsible for\n\nsurveillance of potential disease outbreaks, sound the alert when necessary;\ncreation and sharing of data system and data on cases\nstandardising policy making and training\nevaluation of the capacity of individual countries\ncoordination of personnels\n\nHowever it is difficult even for all countries to reach an agreement, and secure the funds. There is no one organisation that is able to join forces from all parties. Organisations depend on volunteers. WHO lacks funds, experts who are specialised in pandemic research, and relies on the free global response networks.\nA team, GERM (Global Epidemic Response and Mobilisation) should be established. The main task should be disease surveillance and modeling; rather than treating patients.\n\n\n\n\n\nPassive surveillance: healthcare workers report cases that use healthcare services (e.g. clinic, hospital) to public health authorities.\nActive surveillance: workers go to communities to find potential patients that have not been to clinics or hospitals due to inconvenience, or mild symptoms.\nWhen there are many cases (clusters), the signal might be picked up by a computer algorithm, and alerts sent to healthcare workers so that they pay more attention.\nIn some countries, personnels other than healthcare workers (e.g. teachers, post office staff) might also participate in disease surveillance. In addition, some virus can be detected in the environment such as waste water (e.g. polio, illeagal drug).\n\n\n\nIn LMIC (low and middle income countries), there are higher percentage of unrecorded births ad deaths. Some of them carry out census every other year - no real time data. Some data are also lack information such as cause of death. Without knowing causes of death (such as diarrhea), it is impossible to prevent the disease.\nPost-mortem (autopsy) can be unattainable, especially in LMIC. It could also be undesirable for families who have lost their loved ones - procedures are very invasive. Nevertheless, alternatives exist, such as minimally invasive autopsy technologies like MRI and MRI guided fine-needle biopsy.\n\n\n\n\nThe effect of NPI (non-pharmaceutical intervention, such as masks and lockdown) is difficult to quantify; yet it is a very important measure.\nParadox: NPI is effective -> reduced cases -> people think NPI is not necessary\nLockdown can slow down spread, yet it has a huge impact on economy, especially for LMIC.\nContact tracing\n\nnot a new technology; used for smallpox, Ebola, AIDS.\nnot widely applicable: some countries do it better than others. Need more trust from people towards public health agencies.\nsmartphone apps, not very useful: limited by users\n\nGood ventilation system\n\nviruses survive in air, but for different length of time\n\nSocial distance\n\n6 inch isn’t a magic distance\ndepends on circumstances: indoor/outdoor\n\n\n\n\nInfodemic\n(these two chapters are highly technical, and they deserve a separate note)\n\n\n\nDisaster simulation and drill\nDrill: assume a city is at risk of a disease that has epidemic potential\n\nhow to develop diagnostic tests and large-scale manufacturing and distribution\ngovernment, timely and comprehensive information dissemination\nmanagement of quarantine\nset up system for case reporting\n\n\n\n\nThe impact is different among different groups.\nVaccination distribution is highly imbalanced; yet it is only one of the many aspects where inequality exists, and not even the most unequal.\n\n\n\n\n\nInvest in better vaccine, treatment, diagnosis\nTesting and approval process\nFinding new treatment and vaccine\n\ncreate a large database for anti-virus chemicals, open to all\nAI and software to speed-up development\n\n\n\n\nPublic health agencies are under-funded, this is true for all levels: state/county, country and international organizations (WHO).\n\n\n\nImprove census, birth and death in LMIC; then expand into sequencing pathogens, environmental monitoring.\nAggregate disease surveillance systems internationally, and provide real-time data\n\n\n\nRebuild the system after Covid, invest more in healthcare, more staff\nSpend more on basic prevention for all and early diagnosis, rather than in-hospital treatment for severe cases\nManagement, clear tasks and responsibility"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#learn-from-covid",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#learn-from-covid",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "Learn from COVID",
    "text": "Learn from COVID\nSome mistakes\n\nFederal agencies refused to share data\nPersons responsible did not have training inn epidemiology\nNot enough testing, not fast enough\nLacks information sharing"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#create-a-pandemic-prevention-team",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#create-a-pandemic-prevention-team",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "Create a pandemic prevention team",
    "text": "Create a pandemic prevention team\nThere should be a global expert team to help preventing the pandemic: they should be responsible for\n\nsurveillance of potential disease outbreaks, sound the alert when necessary;\ncreation and sharing of data system and data on cases\nstandardising policy making and training\nevaluation of the capacity of individual countries\ncoordination of personnels\n\nHowever it is difficult even for all countries to reach an agreement, and secure the funds. There is no one organisation that is able to join forces from all parties. Organisations depend on volunteers. WHO lacks funds, experts who are specialised in pandemic research, and relies on the free global response networks.\nA team, GERM (Global Epidemic Response and Mobilisation) should be established. The main task should be disease surveillance and modeling; rather than treating patients."
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#get-better-at-detecting-outbreaks-early",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#get-better-at-detecting-outbreaks-early",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "Get better at detecting outbreaks early",
    "text": "Get better at detecting outbreaks early\n\nDisease surveillance\nPassive surveillance: healthcare workers report cases that use healthcare services (e.g. clinic, hospital) to public health authorities.\nActive surveillance: workers go to communities to find potential patients that have not been to clinics or hospitals due to inconvenience, or mild symptoms.\nWhen there are many cases (clusters), the signal might be picked up by a computer algorithm, and alerts sent to healthcare workers so that they pay more attention.\nIn some countries, personnels other than healthcare workers (e.g. teachers, post office staff) might also participate in disease surveillance. In addition, some virus can be detected in the environment such as waste water (e.g. polio, illeagal drug).\n\n\nBirth and death\nIn LMIC (low and middle income countries), there are higher percentage of unrecorded births ad deaths. Some of them carry out census every other year - no real time data. Some data are also lack information such as cause of death. Without knowing causes of death (such as diarrhea), it is impossible to prevent the disease.\nPost-mortem (autopsy) can be unattainable, especially in LMIC. It could also be undesirable for families who have lost their loved ones - procedures are very invasive. Nevertheless, alternatives exist, such as minimally invasive autopsy technologies like MRI and MRI guided fine-needle biopsy."
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#help-people-protect-themselves-right-away",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#help-people-protect-themselves-right-away",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "Help people protect themselves right away",
    "text": "Help people protect themselves right away\nThe effect of NPI (non-pharmaceutical intervention, such as masks and lockdown) is difficult to quantify; yet it is a very important measure.\nParadox: NPI is effective -> reduced cases -> people think NPI is not necessary\nLockdown can slow down spread, yet it has a huge impact on economy, especially for LMIC.\nContact tracing\n\nnot a new technology; used for smallpox, Ebola, AIDS.\nnot widely applicable: some countries do it better than others. Need more trust from people towards public health agencies.\nsmartphone apps, not very useful: limited by users\n\nGood ventilation system\n\nviruses survive in air, but for different length of time\n\nSocial distance\n\n6 inch isn’t a magic distance\ndepends on circumstances: indoor/outdoor"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "Find new treatment fast",
    "text": "Find new treatment fast"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#get-ready-to-make-vaccines",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#get-ready-to-make-vaccines",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "Get ready to make vaccines",
    "text": "Get ready to make vaccines"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#practice-practice-practice",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#practice-practice-practice",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "Practice, practice, practice",
    "text": "Practice, practice, practice\nDisaster simulation and drill\nDrill: assume a city is at risk of a disease that has epidemic potential\n\nhow to develop diagnostic tests and large-scale manufacturing and distribution\ngovernment, timely and comprehensive information dissemination\nmanagement of quarantine\nset up system for case reporting"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#close-the-health-gap-between-rich-and-poor-countries",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#close-the-health-gap-between-rich-and-poor-countries",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "Close the health gap between rich and poor countries",
    "text": "Close the health gap between rich and poor countries\nThe impact is different among different groups.\nVaccination distribution is highly imbalanced; yet it is only one of the many aspects where inequality exists, and not even the most unequal."
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#make-and-fund-a-plan-for-preventing-pandemics",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#make-and-fund-a-plan-for-preventing-pandemics",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "Make and fund a plan for preventing pandemics",
    "text": "Make and fund a plan for preventing pandemics\n\nMake and provide better tools\nInvest in better vaccine, treatment, diagnosis\nTesting and approval process\nFinding new treatment and vaccine\n\ncreate a large database for anti-virus chemicals, open to all\nAI and software to speed-up development\n\n\n\nGERM\nPublic health agencies are under-funded, this is true for all levels: state/county, country and international organizations (WHO).\n\n\nImprove disease surveillance\nImprove census, birth and death in LMIC; then expand into sequencing pathogens, environmental monitoring.\nAggregate disease surveillance systems internationally, and provide real-time data\n\n\nImprove health system\nRebuild the system after Covid, invest more in healthcare, more staff\nSpend more on basic prevention for all and early diagnosis, rather than in-hospital treatment for severe cases\nManagement, clear tasks and responsibility"
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html",
    "href": "blog/technotes_20230225_shinyappsio/index.html",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "",
    "text": "Useful references:"
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html#considerations",
    "href": "blog/technotes_20230225_shinyappsio/index.html#considerations",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Considerations",
    "text": "Considerations\nA few ways to do it: Shiny Server (free), shinyapps.io (free and premium), and professional Rstudio Connect (paid).\nI choose to test out the second option, since it allows more possibilities compared to the free open-source Shiny Server.\nThe free option should allow me to create 5 apps, which is more than enough for personal use. It also allows 25 active hours per month; a note on that at the end."
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html#configuration",
    "href": "blog/technotes_20230225_shinyappsio/index.html#configuration",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Configuration",
    "text": "Configuration\nSign up with GitHub account; or something else. It is possible to change account name afterwards.\nIn Rstudio,\n\nfirst install.packages('rsconnect')\nthen, configure the account. It can be done with rsconnect::setAccountInfo() with information provided in your own shinyapps.io page.\n\nBefore the last step, it is necessary to have an app to deploy!"
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html#create-my-first-shiny-project",
    "href": "blog/technotes_20230225_shinyappsio/index.html#create-my-first-shiny-project",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Create my first shiny project",
    "text": "Create my first shiny project\nHere I use my usual workflow of creating a new R project:\n\nCreate a new repo on GitHub;\nClone the repo locally, by opening a new R project with version control.\n\nNow copy the two R scripts from the demo example:\n\nserver.R\nui.R\n\nTest locally by running shiny::runApp(). This should render the app."
  },
  {
    "objectID": "blog/technotes_20230225_shinyappsio/index.html#deploy-to-shinyapps.io",
    "href": "blog/technotes_20230225_shinyappsio/index.html#deploy-to-shinyapps.io",
    "title": "Testing Shiny app and deploy to shinyapps.io",
    "section": "Deploy to shinyapps.io",
    "text": "Deploy to shinyapps.io\nrsconnect::deployApp() will deploy the app, with an automatically generated url that links to your account.\nThe demo app is deployed here.\n\nNote on active hours\nAfter deployment, the site seems to be active until you shut it down manually; or timeout. The default timeout is 15 minutes, which can be reduced to 5 minutes.\n25 hours per month suggests that I can open the site for 300 times (without manually shuting it down). It might be necessary to start using the paid options, if I have more than one site, or multiple users want to access it …"
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html",
    "href": "blog/technotes_20230111_deployqt/index.html",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "",
    "text": "After you have the public repo, clone it to your local repo."
  },
  {
    "objectID": "blog/technotes_20230220_pkgdown/index.html",
    "href": "blog/technotes_20230220_pkgdown/index.html",
    "title": "R package website with pkgdown",
    "section": "",
    "text": "1. Create the website skeleton.\nBefore editing the details, we need to create the skeleton for the website. It can be done with usethis and pkgdown packages.\nIn R, run this:\nusethis::use_pkgdown()\nThis creates the _pkgdown.yml file, which is the place you configure your site.\nTo view the initial package website, use the following command:\npkgdown::build_site()\nThis creates docs/ directory containing a website\n\nREADME.md becomes the homepage,\ndocumentation in man/ generates a function reference,\nvignettes are rendered into articles/.\n\n\n\n2. Edit the vignette documentation\nMake sure that the vignette index is consistent with Title, otherwise it will not render.\n\n\n3. Build and preview your site\nNow check if the site looks good, and contents are correctly positioned.\npkgdown::preview_site()\npkgdown::build_site()\nYou can also do this to build the site.\npkgdown::build_site_github_pages()\n\n\n4. Deploy site with GitHub Pages\nThere seems to be two options:\n\nusethis::use_pkgdown_github_pages(), this function should take care of everything after pushing changes to GH.\nif you used pkgdown::build_site_github_pages() and pushed everything to GitHub, it might not automatically deploy your site to GH pages. I tried to go to Settings -&gt; Pages -&gt; Deploy from a branch -&gt; main -&gt; /docs, this makes Action deploy your site from the docs folder.\n\ndouble check if you have .nojekyll file\nif a website does not show, check whether you have docs in the .gitignore file; since you are deploying from that folder."
  },
  {
    "objectID": "blog/technotes_20230228_clinreport_part3/index.html",
    "href": "blog/technotes_20230228_clinreport_part3/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/technotes_20230228_clinreport_part3/index.html#principles-and-tools",
    "href": "blog/technotes_20230228_clinreport_part3/index.html#principles-and-tools",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Principles and tools",
    "text": "Principles and tools\nReproducibility: Git (code versioning), dependencies (renv for r package dependencies, Docker for system dependencies)\n\nClean code\nCode comments: not recommended! Better to write code in a way that does not need additional comments.\nDRY: don’t repeat yourself (principle of software development), avoid copy and paste everywhere.\nSRP: single-responsibility prinicple, a function should do one thing: either plot a chart, saves a file, changes variables etc, but not all.\nNaming conventions\n\nReserve dots (.) for S3 methods (print.patient)\nReserve CamelCase for R6 classes or package names (OurPatients)\nUse snake cases (all_patients) for function names and arguments, use verb noun pattern (plot_this())\n\n\n\nCode smells\nA function might be too large: break into smaller ones (e.g. could fit in one screen)\nA function violates SRP: break into smaller ones, and be explicit in what result it is expected to return\nA function with multiple arguments: the scenarios to be tested increase rapidly. Recommended to minimize number of critical function arguments, and break the function into smaller ones.\nBad comments in the code: drop the unnecessary, unclear, outdated comments, write code that are self-explanatory.\n\n\nDevelopment workflow\nCode refactoring: change existing code without its functionality\nTDD: Test-Driven Development\n\nstart with writing a new (failing) test\nwrite code thtat passes the nenw tetst\nrefactor the code\nand repeat\n\nBenefits: your code is covered by tests; you think of testing scenarios first; “fail fast” - can immediately repair the code; more freedom to refactor (improve) the code.\nHow to test\n\nautomatically: CI/CD, after pushing Git commits\nmanually:\n\nrun all unit tests in the package (Build / Test package)\nrun tests in a selected test file (Run Tests)\nrun a single test in Rstudio console\n\n\nHow to check\n\nR CMD CHECK"
  },
  {
    "objectID": "blog/technotes_20230228_clinreport_part3/index.html#writing-robust-statistical-software",
    "href": "blog/technotes_20230228_clinreport_part3/index.html#writing-robust-statistical-software",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Writing robust statistical software",
    "text": "Writing robust statistical software\nImplement complext statistical methods such that the software is reliable, and includes appropriate testing to ensure high quality and validity and ultimately credibility of statistical analysis results.\n\nchoose the right method and understand them\nsolve the core implementation problem with prototype code\n\nNeed to try a few different solutions, compare and select the best one. Might also need to involve domain experts.\n\nspend enough time on planning the design of the R package\n\nDon’t write the package right away; instead define the scope, discuss with users, and design the package.\nStart to draw a flow diagram, align names, arguments and classes; write prototype code.\n\nassume the package will evolve over time\n\nPackages you depend on will change; users will require new features\nWrite tests\n\nunit tests\nintegration tests\n\nMake the package extensible\n\nconsider object oriented package designs\ncombine functions in pipelines\n\nKeep it manageable\n\navoid too many arguments\navoid too large functions"
  },
  {
    "objectID": "blog/technotes_20230228_clinreport_part3/index.html#key-components",
    "href": "blog/technotes_20230228_clinreport_part3/index.html#key-components",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 3",
    "section": "Key components",
    "text": "Key components\n\nDependency management\nInstall dependencies (system/OS level; R packages)\n\nSet repos (can be specified in options()) to e.g. CRAN, BioConductor\nrenv\ncontainer with dependencies pre-installed\n\n\n\nStatic code analysis\n\nLinting (for programmatic and syntax errors) via lintr package\nCode style enforcement via styler package\nSpell checks identifies misspelled words in vignettes, docs and R code via spelling package\n\n\n\nTesting\n\nR CMD build builds R packages as a installable artifact\nR CMD check runs 20+ checks including unit tests, reports errors, warnigns and notes\nTest coverage reports with covr, checks how many lines of code are covered with tests\nR CMD INSTALL tests R package installation\n\n\n\nDocumentation\nAuto-generated docs via Roxygen and pkgdown\n\n\nRelease and deployments\nRelease artifacts and deployments to target systems\n\nChangelog (features, bug fixes) in the NEWS.md\nRelease: create the package with R CMD build. Validation report with thevalidatoR\nPublishing: CRAN, BioConductor"
  },
  {
    "objectID": "blog/blog_20230301_ds_clinreport/index.html",
    "href": "blog/blog_20230301_ds_clinreport/index.html",
    "title": "Course review: making DS work for clinical reporting",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera (course link). It is not necessary to have a paid coursera membership to view the course, everyone could access it.\nIt is a 4 part course released one month ago (Jan/Feb 2023), and it seems that a follow-up will be released in the future.\nOverall I think it strikes a good balance between high-level introduction of the good practices, and examples with how they are implemented. Even though the course focuses on clinical reporting in the pharmaceutical industry, the practices are highly relevant in other sectors as well (e.g. public health, academia, other industries that use open-source software).\nSpecific statistical methods, packages are introduced only at a high-level; which means the course is not for learning how to use this or that packages; but good practice guidelines.\nIn my opinion,\n\nit would be useful if the learner has some experience with software development and/or statistics; otherwise learners might not know how to practice them.\nmost of the examples are related to R packages (understandable), so some experience with R package (use or develop) is useful.\nit could be a very good study material for university students in related subjects.\n\n\n\n\nModule 1 (notes): what the requirements are regarding clinical reporting, what should be done to meet the quality standards;\nModule 2 (notes): DevOps and Agile\nModule 3 (notes): version Control, git workflows, reproducible clinical reporting\nModule 4 (notes): code quality, robust and reusable code, R packages\nModule 5 (notes): risk management with open source software\n\n\n\n\nI have a few years of experience as an R developer and academic researcher in related fields, so not all concepts are new to me. Nevertheless, I still learned quite a bit. For example,\n\n(Module 1) Data and results sharing needs to follow certain standards, such as CDISC; there are different industry standards to follow when it comes to data acquisition, tabulation and analysis (e.g. ADaM)\n(Module 2) Data scientists not only need hard skills, but also soft skills - they need to be able to wear many hats, and be more flexible and resilient.\n(Module 4, 5) Tests are extremely important. Think afar, develop your package so that they can be extended in the future. Design your package first, don’t start making your package immediately."
  },
  {
    "objectID": "blog/blog_20230104_qtwAcademic/index.html",
    "href": "blog/blog_20230104_qtwAcademic/index.html",
    "title": "qtwAcademic: a quick and easy way to start your Quarto website",
    "section": "",
    "text": "qtwAcademic stands for Quarto Websites for Academics, which provides a few Quarto templates for Quarto website that are commonly used by academics.\nThe templates are designed to make it quick and easy for users with little or no Quarto experience to create a website for their personal portfolio or courses. Each template is fully customizable once the user is more familiar with Quarto.\nRead more about the package here.\nMore details about the package is being written …"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html",
    "href": "blog/technotes_20230301_clinreport_part4/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html#open-source-packages",
    "href": "blog/technotes_20230301_clinreport_part4/index.html#open-source-packages",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Open source packages",
    "text": "Open source packages\nExmample:\n\nsurvival: 8 developers, >18 years\nadmiral: 25 developers, >1 year\ntern: 77 developers, 5 years\nrtables: 21 developers, 4 years\n\nEngagement across these packages is different, some receive more issues and comments, some receive more code contributions.\nStale: stable? abandoned?\nContribution is highly skewed, a few contributors write the majority of the code.\nR package life cycles (indicative, not guaranteed)\n\nexperimental (ready to use?)\nstable (safe to use?)\ndeprecated, no longer maintained\nsuperseded, something better exists\n<1.0: big changes likely; >=v1.0: is it safe to use?"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html#risk-mitigation-for-r-packages",
    "href": "blog/technotes_20230301_clinreport_part4/index.html#risk-mitigation-for-r-packages",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Risk mitigation for R packages",
    "text": "Risk mitigation for R packages\nCombine external and internal packages (CI/CD release)\n-> automated package data collection\n-> automated quality checks: if not pass, assess\n-> package repo integration tests\n-> publish to package repo, generate package validation report"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html#assess-external-packages-for-statistical-methods",
    "href": "blog/technotes_20230301_clinreport_part4/index.html#assess-external-packages-for-statistical-methods",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Assess external packages for statistical methods",
    "text": "Assess external packages for statistical methods\nDoes it provide the required functionality?\n\nCorrect statistical method?\nCould you extend it?\nCorrect results? (compare with another software)\nDo you understand the method? (check the paper linked with package)\n\nDoes it work reliably?\n\nPublished? (e.g. on CRAN)\nDifferent inputs?\nFast?\nDo other people use it? (downloads)\nDoes other software use it? (reverse dependencies)\n\nDoes the code look robust and well tested?\n\nHow are the functions implemented\nIs the source code readable\nCoverage with unit tests\nMature package?\n\nIs it well documented?\n\nDocumented functions?\nVignettes?\nPublished?\nInformative NEWS entry?\n\nWho are the authors, are they responsive?\n\nDid they publish statistics papers on this topic\nIs a github site with issues available"
  },
  {
    "objectID": "blog/technotes_20230301_clinreport_part4/index.html#tools",
    "href": "blog/technotes_20230301_clinreport_part4/index.html#tools",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 4",
    "section": "Tools",
    "text": "Tools\ncovr and unit tests\nriskmetric and the R Validation Hub\npharmaverse.org, with end-to-end examples"
  },
  {
    "objectID": "blog/technotes_20230222_clinreport_part2/index.html",
    "href": "blog/technotes_20230222_clinreport_part2/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/technotes_20230222_clinreport_part2/index.html#agile-mindset-and-devops-practices",
    "href": "blog/technotes_20230222_clinreport_part2/index.html#agile-mindset-and-devops-practices",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Agile mindset and DevOps practices",
    "text": "Agile mindset and DevOps practices\n\nData science as a new way of thinking\nNew way of working means\n\nleverage standards and automation (CI/CD)\nadopt new data types quickly, reusing data for multiple purposes, pooling data, data marts\nopen-sourcing and collaborating cross pharma (small, readable, self-tested code)\ncoding for reusability, moving away from single-use programs\nrapidly re-arranginng re-usable components to meet analytical need at hand\n\nData scientist need to have hard skills, such as\n\nSAS, R, Python, JS, bash\ncloud, containers\nCI/CD tools\nvisualisation\nknowledge of various data types\n\nand also soft skills:\n\ncollaborative and inclusive\ntransparent and practical\ncreative and proactive\nasking the right questions\nable to wear many hats, be more flexible and resilient\n\n\n\nAgile\nProject management; a mindset: uncover better ways of working, by doing and helping others do it.\n1st principle: highest priority is to satisfy the customer through early and continuous delivery of valuable software.\nImplementations: Kanban, Scrum, Lean, Extreme programming\nTools:\n\nbacklog\nkanban board (not started, in progress, done)\nWIP (work in progress limit)\nprogress measures: e.g. team velocity\n\n\n\nDevOps\nIncrease efficiency by improving the connection between Dev (software development) and Ops (IT operations).\nThe goal is continuous delivery and continuous improvement.\nPractices:\n\nmodular architecture\nversion control\nmerge into trunk daily\nautomated and continuous testing, continuous integration\nautomated deployment\n\n\nDevOps in clinical reporting\nRisks around production run:\n\nare all dependencies in production?\nwas all quality control completed and successful?\nis all documentation complete?\nwas the transfer to eDMS correct and successful?"
  },
  {
    "objectID": "blog/technotes_20230222_clinreport_part2/index.html#version-control",
    "href": "blog/technotes_20230222_clinreport_part2/index.html#version-control",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Version control",
    "text": "Version control\nFeature branch (as opposed to master branch): one task per branch\nname feature branch: issue number and description\nEach issue should have a clear description, short and specific; instead of being long and overarching.\n\nWorkflow for clinical reporting\nRestraints of clinical deliveries: timing annd multiple deliveries; resourcing challenges\nMight need to choose between feature and GitFlow."
  },
  {
    "objectID": "blog/technotes_20230222_clinreport_part2/index.html#reproducible-projects-in-r",
    "href": "blog/technotes_20230222_clinreport_part2/index.html#reproducible-projects-in-r",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 2",
    "section": "Reproducible projects in R",
    "text": "Reproducible projects in R\nTo reproduce your work:\n\nGit (version control)\nR libraries\nWell structured projects\nUnderlying dependencies (e.g. operating systems, C++/C)\n\n\nWell structured projects\nClear names\nGood documentation\n\n\nR libraries and versions\nCheck session info; but not the most practical way.\nUse global libraries, .libPaths(), this gives you the path where all the packages are installed. Global libraries is useful when using a server for multiple R sessions, where they look for the packages in the same place.\nSolutions\n\nrenv package: makes each project in R self-contained.\nCheckpoint: project level library paths based on snapshots of CRAN\n\nUse Docker images! Saves R version, operating system, underlying dependencies"
  },
  {
    "objectID": "blog/technotes_20230205_clinreport_part1/index.html",
    "href": "blog/technotes_20230205_clinreport_part1/index.html",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "",
    "text": "This is a course provided by Genentech (part of Roche) on Coursera.\nCourse link"
  },
  {
    "objectID": "blog/technotes_20230205_clinreport_part1/index.html#introduction-to-clinical-trial",
    "href": "blog/technotes_20230205_clinreport_part1/index.html#introduction-to-clinical-trial",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "Introduction to clinical trial",
    "text": "Introduction to clinical trial\nClinical trial: aim to demonstrate that drug is safe and effective (safety, efficacy)\n\nphase 1: 10-20 people, focus on safety (healthy volunteers)\nphase 2: 100, study of side effects, determine best dose\nphase 3: 1000, demonstrate drug efficacy, fuller safety profile (common across multiple regions, ethnicities)\n\ncollecting data from different hospitals, hence important to ensure standards are being followed\n\nevidence must be submitted to health authorities (FDA, EMA European medicines agency)\nhealth authorities determine whether the drug is submitted to market\n\nSubmit the analysis plan in advance"
  },
  {
    "objectID": "blog/technotes_20230205_clinreport_part1/index.html#why-share-data",
    "href": "blog/technotes_20230205_clinreport_part1/index.html#why-share-data",
    "title": "Notes: Making Data Science work for Clinical Reporting - Part 1",
    "section": "Why share data",
    "text": "Why share data\n\nregulatory requirements\nscientific community interest\ncompany internal research interest\nmarketing materials\n\n\nData and results sharing\n\nRegulatory req (e.g. EMA require sharing clinical trial results to gain marketing authorization for pharma products, FDA require sharing data)\nscientific community (peer review check accuracy, perform additional analyses, derive new hypothesis)\nCDISC standards\n\nCDASH (clinical data acquisition standards harmonization)\nSDTM (study data tabulation model)\n\nformat for ‘raw’ data, define datasets, structures, contents, variable attributes\n\nSEND (standard for exchange of non clinical data)\nADaM (analysis data model)\n\ndata format for data processed for analysis (e.g. converted, imputed, derived)\n\n\nDictionary\n\ne.g. nose congestion, stuffy nose, … need to be standardized\nMedDRA: standard dictionary for medical conditions, events and procedures\nWHO drug dictionary (for pharma agents)\n\nSAP statistical analysis plan\n\nbased on study protocol, focus on statistical methodology, is regulated\n\nProgramming specification\n\nbased on SAP, provides additional details on datasets and tables, listing and figures (TLFs) required for statistical analysis. focus on programming details. Not regulated\n\n\n\n\nQuality assurance\n\nGood clinical practice (GCP), issued by ICH\npurpose: prevent mistakes, reduce inefficiencies/waste in a process, increase reliability/trustworthiness of the product of a process\nClinical monitoring: performed by a clinical research assistant (CRA) at investigator sites, checks that study protocols are executed as intended, and site processes result in accurate data capture. Focus on trial subjects’ safety. Traditional goal: 100% source data verification\nData quality checks (more relevant for data scientists). Checks data for technical conformance, and data plausibility. Focus on data quality. Traditional goal: 100% accurate and format compliant data\nCode review\nDouble programming\n\n\n\nData access restrictions\n\nreasons\n\ndata collected is very sensitive (health data), need data protection\nclinical trial data is a key asset and revenue predictor for pharma companies, high confidentiality levels\nscientific validity, data is ideally double blinded, no-one should know whether a subjecttreatment is as long as the data is still being collected\n\npseudonymization: data de-identification\n\nuse pseudonym (ID), link is recorded to allow re-identification\n\nanonymization: limit the risk of re-identification\n\nremove variables, remove values, replace more precise values with more general categories, replace personal identifiers with random identifiers\n\nFSP, CRO (out-sourcing), personnels require data access at different levels\nUnblinding"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Blog and notes",
    "section": "",
    "text": "Blog\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nPersonal Highlights: Positconf 2023\n\n\n9/21/23\n\n\n\n\nPersonal Highlights: CEN2023\n\n\n9/6/23\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\n7/17/23\n\n\n\n\nCourse review: making DS work for clinical reporting\n\n\n3/1/23\n\n\n\n\nOpen source reporting with R: clinical, public health, RSE and embrace the change\n\n\n1/13/23\n\n\n\n\nqtwAcademic: a quick and easy way to start your Quarto website\n\n\n1/5/23\n\n\n\n\nWebsite reboot: switching from Blogdown to Quarto\n\n\n1/3/23\n\n\n\n\n\nNo matching items\n\n\n\n\nTechnical notes\nMost of the technical notes are in the newly built note repository, Data Apothecary’s Notes. Please feel free to reach out if you found any errors!\nI’d be glad if it helps you in some way.\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nNotes: The Book of OHDSI - Data Analytics\n\n\n5/6/24\n\n\n\n\nStyling your quarto project\n\n\n10/18/23\n\n\n\n\nUse WebR in your existing quarto website\n\n\n10/1/23\n\n\n\n\nR package workflow\n\n\n5/19/23\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 4\n\n\n3/1/23\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 3\n\n\n2/27/23\n\n\n\n\nTesting Shiny app and deploy to shinyapps.io\n\n\n2/25/23\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 2\n\n\n2/22/23\n\n\n\n\nR package website with pkgdown\n\n\n2/20/23\n\n\n\n\nNotes: Making Data Science work for Clinical Reporting - Part 1\n\n\n2/6/23\n\n\n\n\nPublishing Quarto Website with GitHub Pages\n\n\n1/11/23\n\n\n\n\n\nNo matching items\n\n\n\n\nReading notes\nThis section is constantly being updated.\n\n\n\n\n\nTitle\n\n\nDate\n\n\n\n\n\n\nBad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre\n\n\n6/6/24\n\n\n\n\nWorking in Public: The Making and Maintenance of Open Source Software - Nadia Eghbal\n\n\n2/18/24\n\n\n\n\nPreventable: How a Pandemic Changed the World & How to Stop the Next One - Devi Sridhar\n\n\n3/17/23\n\n\n\n\nHow to prevent the next pandemic - Bill Gates\n\n\n1/4/23\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/technotes_20230223_roop/index.html",
    "href": "blog/technotes_20230223_roop/index.html",
    "title": "OOP in R: S3",
    "section": "",
    "text": "Useful references:"
  },
  {
    "objectID": "blog/technotes_20230223_roop/index.html#terminology",
    "href": "blog/technotes_20230223_roop/index.html#terminology",
    "title": "OOP in R: S3",
    "section": "Terminology",
    "text": "Terminology\n\nObject: individual instances of a class\nClass: type of an object, i.e. what an object is\nMethod: a function associated with a particular class, i.e. what the object can do\n\ngeneric method: mean() of a vector of numbers is a number, mean() of a vector of dates is a date\nInherit: a sub-class inherits all the attributes and methods from the super-class. E.g. generalized linear model inherits from a linear model.\nmethod dispatch: the process of finding the correct method given a class\n\n\nEncapsulated OOP:\n\nmethods belong to object or classes\nobject.method(arg1, arg2)\ncommon in most languages\nR6, RC (reference class) are examples of this type\n\nFunctional OOP:\n\nmethods belong to generic functions\ngeneric(object, arg2, arg3)\nS3 is an informal implementation of this type"
  },
  {
    "objectID": "blog/technotes_20230223_roop/index.html#base-types",
    "href": "blog/technotes_20230223_roop/index.html#base-types",
    "title": "OOP in R: S3",
    "section": "Base types",
    "text": "Base types\nCheck whether an object is object-oriented, or base object:\n\nis.object()\nsloop::otype(): returns base or S3/S4\nattr(obj_name, 'class'): OO objects has a class attribute, BO does not.\n\n\nx <- 1:10 # a numeric vector\ny <- factor(c('a', 'b'))  # a factor\n\nc(is.object(x), is.object(y))\n\n[1] FALSE  TRUE\n\nc(sloop::otype(x), sloop::otype(y))\n\n[1] \"base\" \"S3\"  \n\nattr(x, 'class') \n\nNULL\n\nattr(y, 'class')\n\n[1] \"factor\"\n\n\nAll objects have a base type; not all are OO objects.\n\ntypeof(1:10) returns ‘integer’\n25 base types in total\n\nvectors: e.g. NULL, logical, integer, double, complex, character, list, raw\nfunctions: e.g. closure, special, builtin\nenvironments: environment\nS4: S4\nlanguage components, symbol, language, pairlist the rest are less common."
  },
  {
    "objectID": "blog/technotes_20230223_roop/index.html#generic-or-method",
    "href": "blog/technotes_20230223_roop/index.html#generic-or-method",
    "title": "OOP in R: S3",
    "section": "Generic or method?",
    "text": "Generic or method?\n\ngeneric.class(), for example: print.factor()\ndo not call the method directly; use the generic (dispatch) to find it.\ngenerally has the . in the name; however it is not guaranteed * t.test() is a generic like print(), as t.test() can be used on multiple types of inputs\n\nas.factor() is not an OO object, hence not S3\n\n\n\nCheck function type with sloop::ftype()\n\nsloop::ftype(predict) # predict is a generic\n\n[1] \"S3\"      \"generic\"\n\nsloop::ftype(predict.glm)  # glm (class) method for predict() generic\n\n[1] \"S3\"     \"method\"\n\n\n\n\nCheck methods with methods()\nmethods() checks all the methods that either:\n\nbelongs to a generic (the function), such as plot, predict, t.test\nbelongs to a class (the type of input), such as lm, ar\n\n\nmethods('predict')  \n\n [1] predict.ar*                predict.Arima*            \n [3] predict.arima0*            predict.glm*              \n [5] predict.HoltWinters*       predict.lm*               \n [7] predict.loess*             predict.mlm*              \n [9] predict.nls*               predict.poly*             \n[11] predict.ppr*               predict.prcomp*           \n[13] predict.princomp*          predict.smooth.spline*    \n[15] predict.smooth.spline.fit* predict.StructTS*         \nsee '?methods' for accessing help and source code\n\nmethods(class = 'lm')\n\n [1] add1           alias          anova          case.names     coerce        \n [6] confint        cooks.distance deviance       dfbeta         dfbetas       \n[11] drop1          dummy.coef     effects        extractAIC     family        \n[16] formula        hatvalues      influence      initialize     kappa         \n[21] labels         logLik         model.frame    model.matrix   nobs          \n[26] plot           predict        print          proj           qr            \n[31] residuals      rstandard      rstudent       show           simulate      \n[36] slotsFromS3    summary        variable.names vcov          \nsee '?methods' for accessing help and source code\n\n\nEquivalently, use sloop::s3_methods_*(), as it gives more information in the output.\n\nsloop::s3_methods_generic('predict') \n\n# A tibble: 16 × 4\n   generic class             visible source             \n   <chr>   <chr>             <lgl>   <chr>              \n 1 predict ar                FALSE   registered S3method\n 2 predict Arima             FALSE   registered S3method\n 3 predict arima0            FALSE   registered S3method\n 4 predict glm               FALSE   registered S3method\n 5 predict HoltWinters       FALSE   registered S3method\n 6 predict lm                FALSE   registered S3method\n 7 predict loess             FALSE   registered S3method\n 8 predict mlm               FALSE   registered S3method\n 9 predict nls               FALSE   registered S3method\n10 predict poly              FALSE   registered S3method\n11 predict ppr               FALSE   registered S3method\n12 predict prcomp            FALSE   registered S3method\n13 predict princomp          FALSE   registered S3method\n14 predict smooth.spline     FALSE   registered S3method\n15 predict smooth.spline.fit FALSE   registered S3method\n16 predict StructTS          FALSE   registered S3method\n\nsloop::s3_methods_class('lm')\n\n# A tibble: 35 × 4\n   generic        class visible source             \n   <chr>          <chr> <lgl>   <chr>              \n 1 add1           lm    FALSE   registered S3method\n 2 alias          lm    FALSE   registered S3method\n 3 anova          lm    FALSE   registered S3method\n 4 case.names     lm    FALSE   registered S3method\n 5 confint        lm    FALSE   registered S3method\n 6 cooks.distance lm    FALSE   registered S3method\n 7 deviance       lm    FALSE   registered S3method\n 8 dfbeta         lm    FALSE   registered S3method\n 9 dfbetas        lm    FALSE   registered S3method\n10 drop1          lm    FALSE   registered S3method\n# ℹ 25 more rows"
  },
  {
    "objectID": "rpkg/cstime/index.html",
    "href": "rpkg/cstime/index.html",
    "title": "cstime",
    "section": "",
    "text": "cstime provides convenient and consistent conversion between\n\nisoyear\nisoweek\ncalyear\nseason week (used in influenza surveillance)\n\nGitHub link"
  },
  {
    "objectID": "rpkg/medicode/index.html",
    "href": "rpkg/medicode/index.html",
    "title": "medicode",
    "section": "",
    "text": "This package provides metadata and tools for medical classification and clinical coding.\nGitHub Link\nIt is especially useful for English and Norwegian (Bokmål) languages.\nPlanned content:\n\nICD-10\nICPC-2\nEuropean shortlist for Cause of Death"
  },
  {
    "objectID": "rpkg/csdata/index.html",
    "href": "rpkg/csdata/index.html",
    "title": "csdata",
    "section": "",
    "text": "https://github.com/csids/csdata"
  },
  {
    "objectID": "rpkg/rpkg.html",
    "href": "rpkg/rpkg.html",
    "title": "R packages",
    "section": "",
    "text": "Title\n\n\nDescription\n\n\n\n\n\n\ncovidnor\n\n\nOpen COVID19 data for Norway\n\n\n\n\ncsalert\n\n\nOutbreak detection in public health surveillance\n\n\n\n\ncsdata\n\n\nPre-formatted structural data for Norway\n\n\n\n\ncsmaps\n\n\nPre-formatted map data in Norway\n\n\n\n\ncstime\n\n\nDate and time functions for public health purposes\n\n\n\n\nnoreden\n\n\nTools to facilitate nutrition researcher for sustainable diet discovery\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "rpkg/qtwAcademic/index.html",
    "href": "rpkg/qtwAcademic/index.html",
    "title": "qtwAcademic",
    "section": "",
    "text": "qtwAcademic stands for Quarto Websites for Academics, which provides a few Quarto templates for Quarto website that are commonly used by academics.\nThe templates are designed to make it quick and easy for users with little or no Quarto experience to create a website for their personal portfolio or courses. Each template is fully customizable once the user is more familiar with Quarto.\nRead more about the package here.\n\nTemplates\nSo far, 3 templates have been implemented in this package:\n\nPersonal website\nWebsite for courses or workshops\nMinimal website template that can be easily customized\n\nYou can find more details on each option in the vignettes."
  },
  {
    "objectID": "rpkg/csalert/index.html",
    "href": "rpkg/csalert/index.html",
    "title": "csalert",
    "section": "",
    "text": "https://github.com/csids/csalert"
  },
  {
    "objectID": "rpkg/csmaps/index.html",
    "href": "rpkg/csmaps/index.html",
    "title": "csmaps",
    "section": "",
    "text": "https://github.com/csids/csmaps"
  },
  {
    "objectID": "rpkg/mortanor/index.html",
    "href": "rpkg/mortanor/index.html",
    "title": "mortanor",
    "section": "",
    "text": "https://github.com/csids/mortanor"
  },
  {
    "objectID": "rpkg/covidnor/index.html",
    "href": "rpkg/covidnor/index.html",
    "title": "covidnor",
    "section": "",
    "text": "https://github.com/csids/covidnor"
  },
  {
    "objectID": "rpkg/nowcast/index.html",
    "href": "rpkg/nowcast/index.html",
    "title": "nowcast",
    "section": "",
    "text": "https://github.com/csids/nowcast"
  },
  {
    "objectID": "rpkg/bayesynergy/index.html",
    "href": "rpkg/bayesynergy/index.html",
    "title": "bayesynergy",
    "section": "",
    "text": "An R package for Bayesian semi-parametric modelling of in-vitro drug combination experiments\nGitHub Link"
  },
  {
    "objectID": "teaching/teaching.html",
    "href": "teaching/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "I started teaching introductory statistics at Faculty of Medicine, University of Oslo since early 2023. I aim to make basic data skills and statistics accessible to health researchers, by combining modern data science tools (R, quarto, version control) with hands-on practice in my class.\nIn my free time I volunteer in the Carpentries workshops to teach Rstats to researchers and students at UiO."
  },
  {
    "objectID": "teaching/teaching.html#workshops",
    "href": "teaching/teaching.html#workshops",
    "title": "Teaching",
    "section": "Workshops",
    "text": "Workshops\n\n\n\n\n\nTitle\n\n\nDescription\n\n\ntype\n\n\n\n\n\n\nOslo Bioinformatics Week\n\n\nMachine Learning principles for small and noisy biomedical data\n\n\nbioinformatics\n\n\n\n\nR for Reproducible Scientific Analysis\n\n\nSoftware carpentry course for introductory R\n\n\nrstats\n\n\n\n\nZero to Quarto Workshop\n\n\nZurich R Ladies workshop\n\n\nquarto\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/teaching.html#guides-and-cheatsheets",
    "href": "teaching/teaching.html#guides-and-cheatsheets",
    "title": "Teaching",
    "section": "Guides and Cheatsheets",
    "text": "Guides and Cheatsheets\n\n\n\nTitle\nDescription\n\n\n\n\nggplot2\nLecture notes for ggplot2 at the Carpentries workshop, UiO\n\n\nlist of commands\nQuick reference list of R commands used in my statistics classes"
  },
  {
    "objectID": "teaching/cheatsheet_pkgdev/index.html",
    "href": "teaching/cheatsheet_pkgdev/index.html",
    "title": "Pkg Dev at CSIDS",
    "section": "",
    "text": "Rstat"
  },
  {
    "objectID": "teaching/mf9130/index.html",
    "href": "teaching/mf9130/index.html",
    "title": "MF9130/MF9130E",
    "section": "",
    "text": "This 8-days course is provided in both English and Norwegian. Generally,"
  },
  {
    "objectID": "teaching/mf9130/index.html#mf9130---innføring-i-statistikk",
    "href": "teaching/mf9130/index.html#mf9130---innføring-i-statistikk",
    "title": "MF9130/MF9130E",
    "section": "MF9130 - Innføring i statistikk",
    "text": "MF9130 - Innføring i statistikk\nMF9130\nMålet for kurset er å gjøre deltagerne kjent med grunnleggende statistiske ideer og metoder. Det forutsettes ikke spesielle forkunnskaper i matematikk eller statistikk. Den statistiske programpakken STATA vil bli benyttet i mange av øvelsene. Analyse av konkrete eksempler fra medisinsk forskning vil bli vektlagt."
  },
  {
    "objectID": "teaching/mf9130/index.html#mf9130e---introductory-course-in-statistics",
    "href": "teaching/mf9130/index.html#mf9130e---introductory-course-in-statistics",
    "title": "MF9130/MF9130E",
    "section": "MF9130E - Introductory course in statistics",
    "text": "MF9130E - Introductory course in statistics\nCourse site developed by instructors (I am the lead developer for this website)\nOfficial course site: MF9130E\nThe aim of the course is to make the participants acquainted with basic statistical ideas and methods. No special previous knowledge of mathematics or statistics is assumed. The statistical software R and the RStudio environment will be used in many of the exercises. Analysis of examples from biomedical research will be emphasized."
  },
  {
    "objectID": "teaching/r_novice/index.html",
    "href": "teaching/r_novice/index.html",
    "title": "R for Reproducible Scientific Analysis",
    "section": "",
    "text": "I have been involved in multiple introductory R workshops at University of Oslo, sometimes as the instructor and sometimes, a helper.\nYou can check the course material for my ggplot2 lecture.\nMore information about the Carpentries at University of Oslo can be found here: Carpentries at UiO"
  },
  {
    "objectID": "teaching/workshop_bioinf/index.html",
    "href": "teaching/workshop_bioinf/index.html",
    "title": "Oslo Bioinformatics Week",
    "section": "",
    "text": "This workshop is part of the Oslo Bioinformatics Workshop Week 2022. These workshops are open to the scientific community in Oslo and the surrounding area.\nMain instructor: Manuela Zucknick\nCo-instructor: Theophilius Asenso\nDeveloper: Chi Zhang\nCourse website and repository"
  },
  {
    "objectID": "about_publication.html",
    "href": "about_publication.html",
    "title": "Chi Zhang",
    "section": "",
    "text": "Twitter\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github\n  \n\n  \n  \nChi is a researcher at the Norwegian Institute of Public Health (Folkehelseinstituttet). She is a statistician and R programmer at Sykdomspulsen, a real-time analysis and public health surveillance system.\nChi has recently earned her PhD at University of Oslo. Her thesis is Representation and utilization of hospital Electronic Health Records data.\nChi also has a 20% position as a statistical advisor at the Oslo Centre for Biostatistics and Epidemiology (OCBE).\nYou can check my CV here. (add CV!)\nthis is another one"
  },
  {
    "objectID": "blog/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast-get-readyy-to-make-vaccines",
    "href": "blog/readnotes_2023010x_pandemic_gates/index.html#find-new-treatment-fast-get-readyy-to-make-vaccines",
    "title": "How to prevent the next pandemic - Bill Gates",
    "section": "Find new treatment fast, get readyy to make vaccines",
    "text": "Find new treatment fast, get readyy to make vaccines\nInfodemic\n(these two chapters are highly technical, and they deserve a separate note)"
  },
  {
    "objectID": "blog/technotes_20230223_roop/index.html#define-generic-and-method",
    "href": "blog/technotes_20230223_roop/index.html#define-generic-and-method",
    "title": "OOP in R: S3",
    "section": "Define generic and method",
    "text": "Define generic and method\n[name of method] <- functionn(x){UseMethod(\"[name of method]\")}\nNow we define one generic function f, and two methods. One for class plus2, and another for class plus10.\n\nf <- function(x){UseMethod('f')} # define generic f\nf.plus2 <- function(x) x+2 # f method for class plus2\nf.plus10 <- function(x) x+10 # f method for class plus10\n\nNow we try to give the function some input. First use a numeric number, 1 (the class for a number is double and numeric).\n\nnumber <- 1\nf(number) # returns error, class of number does not match!\n\nError in UseMethod(\"f\"): no applicable method for 'f' applied to an object of class \"c('double', 'numeric')\"\n\n\nThis returns an error, because the class of number is not defined for function f (plus2, plus10).\n\n# can check what f(number) tried \n# none of these exist \nsloop::s3_dispatch(f(number))\n\n   f.double\n   f.numeric\n   f.default\n\n\nWe need to match it.\n\n# fix: assign a class to number\nclass(number) <- 'plus2'\nf(number) # number+2, f.plus2 method\n\n[1] 3\nattr(,\"class\")\n[1] \"plus2\""
  },
  {
    "objectID": "blog/technotes_20230223_roop/index.html#class-assignment",
    "href": "blog/technotes_20230223_roop/index.html#class-assignment",
    "title": "OOP in R: S3",
    "section": "Class assignment",
    "text": "Class assignment\nTwo options: structure(), or class(existing_obj)\n\nsimple_number <- structure(1, class = 'simple')\nclass(simple_number)\n\n[1] \"simple\"\n\n\nOr, you can do it for an existing object by giving it a class\n\nsimple_char <- 'your_name'\nclass(simple_char) <- 'simple'\nclass(simple_char)\n\n[1] \"simple\""
  },
  {
    "objectID": "blog/technotes_20230223_roop/index.html#constructor",
    "href": "blog/technotes_20230223_roop/index.html#constructor",
    "title": "OOP in R: S3",
    "section": "Constructor",
    "text": "Constructor\n\nfruit <- function(x){\n  stopifnot(is.character(x))\n  # checks if x is char\n  # better use a named list, easier to call\n  structure(list(fruit_name = x), class = 'fruit') \n}\n\nfruit1 <- fruit('pineapple')\nfruit2 <- fruit('apple')\n\nExamine what comes out\n\nfruit1\n\n$fruit_name\n[1] \"pineapple\"\n\nattr(,\"class\")\n[1] \"fruit\""
  },
  {
    "objectID": "blog/technotes_20230223_roop/index.html#define-new-generic-and-method",
    "href": "blog/technotes_20230223_roop/index.html#define-new-generic-and-method",
    "title": "OOP in R: S3",
    "section": "Define new generic and method",
    "text": "Define new generic and method\n[name of method] <- functionn(x){UseMethod(\"[name of method]\")}\nNow we define one generic function f, and two methods. One for class plus2, and another for class plus10.\n\nf <- function(x){UseMethod('f')} # define generic f\nf.plus2 <- function(x) x+2 # f method for class plus2\nf.plus10 <- function(x) x+10 # f method for class plus10\n\nNow we try to give the function some input. First use a numeric number, 1 (the class for a number is double and numeric).\n\nnumber <- 1\nf(number) # returns error, class of number does not match!\n\nError in UseMethod(\"f\"): no applicable method for 'f' applied to an object of class \"c('double', 'numeric')\"\n\n\nThis returns an error, because the class of number is not defined for function f (plus2, plus10).\n\n# can check what f(number) tried \n# none of these exist \nsloop::s3_dispatch(f(number))\n\n   f.double\n   f.numeric\n   f.default\n\n\nWe need to match it. Assign the number with plus2 class, and evaluate it. You can check which method has been used (dispatched).\n\n# fix: assign a class to number\nclass(number) <- 'plus2'\nf(number) # number+2, f.plus2 method\n\n[1] 3\nattr(,\"class\")\n[1] \"plus2\"\n\nsloop::s3_dispatch(f(number))\n\n=> f.plus2\n   f.default\n\n\nNow we try another number, but let it be plus10 class.\n\nnumberx <- 200\nclass(numberx) <- 'plus10'\nf(numberx)\n\n[1] 210\nattr(,\"class\")\n[1] \"plus10\"\n\nsloop::s3_dispatch(f(numberx))\n\n=> f.plus10\n   f.default"
  },
  {
    "objectID": "blog/technotes_20230223_roop/index.html#new-method-for-existing-generic-print",
    "href": "blog/technotes_20230223_roop/index.html#new-method-for-existing-generic-print",
    "title": "OOP in R: S3",
    "section": "New method for existing generic (print())",
    "text": "New method for existing generic (print())\nWe create the S3 object using the constructor defined above, fruit().\n\npineapple <- fruit('pineapple') # create by the constructor\npineapple\n\n$fruit_name\n[1] \"pineapple\"\n\nattr(,\"class\")\n[1] \"fruit\"\n\n\nThe output does not look very nice, we can modify what prints out. Since print() is an exisiting generic function, we do not need to define a new one (i.e. UseMethod). We define the new method directly: generic.your_class.\n\n# we do not need to define print() as generic, bec it IS already\n# directly define print.fruit\nprint.fruit <- function(x){\n  cat('I used constructor for my fruit:', x$fruit_name)\n}\n\nprint.fruit(pineapple)\n\nI used constructor for my fruit: pineapple"
  },
  {
    "objectID": "teaching/ern4110/index.html",
    "href": "teaching/ern4110/index.html",
    "title": "ERN4110",
    "section": "",
    "text": "ERN4110\nEmnet gir en anvendt rettet innføring i statistiske grunnbegreper og sentrale statistiske metoder. Det blir lagt vekt på konkrete eksempler fra ernæringsvitenskap og medisin. PC-øvelser med statistikkpakken Stata er en essensiell del av undervisningen."
  },
  {
    "objectID": "blog/statnotes_20230516_survival_part1/index.html",
    "href": "blog/statnotes_20230516_survival_part1/index.html",
    "title": "Survival Analysis - Part I",
    "section": "",
    "text": "Overview"
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html",
    "href": "blog/technotes_20230519_pkgcran/index.html",
    "title": "R package workflow",
    "section": "",
    "text": "This checklist is being updated over time. Mostly for my own use; but great if it helps you as well!\nFor a complete treatment, please refer to R Packages (2e) by Hadley Wickham and Jennifer Bryan."
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#where-things-are",
    "href": "blog/technotes_20230519_pkgcran/index.html#where-things-are",
    "title": "R package checklist",
    "section": "Where things are",
    "text": "Where things are"
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#documentation",
    "href": "blog/technotes_20230519_pkgcran/index.html#documentation",
    "title": "R package checklist",
    "section": "Documentation",
    "text": "Documentation\n\nSufficient, working examples\n\nREADME.md\nHelp pages\nVignettes\n\nExample code coverage, covr::package_coverage(), use GHA workflow to check if code coverage is above a threshold.\nDetect broken README examples by generating README.md on every commit.\nIn help pages, some examples have tags:\n\n\\dontrun{}: not run by example(), not run by R CMD check\n\\donttest{}: run by example() but not checked\n\\dontshow{}: run and checked\n\n\n# code coverage\ncovr::package_coverage(type = c('examples', 'vignettes'), commentDonttest = F, commentDontrun = F)\n\n# readme examples\nrmarkdown::render(\"README.rmd\", output_format = rmarkdown::github_document())\n\n# help page examples\ndevtools::run_examples(run_dontrun = T, run_donttest = T)\n\n# check vignette examples\npkgdown::build_site()\n\n\n\nLinks and spelling errors\nDetect link rot with urlchecker::url_check()\nSpelling, check with spelling::spell_check_package() or usethis::use_spell_check().\n\nspecify prefered standard in DESCRIPTION\ncreate a list of allowed misspelt words, put it under inst."
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#handling-exceptions",
    "href": "blog/technotes_20230519_pkgcran/index.html#handling-exceptions",
    "title": "R package checklist",
    "section": "Handling exceptions",
    "text": "Handling exceptions\nError > warning > message"
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#code-quality",
    "href": "blog/technotes_20230519_pkgcran/index.html#code-quality",
    "title": "R package checklist",
    "section": "Code quality",
    "text": "Code quality\nStyle guide, styler::style_pkg() enforces tidyverse style guide.\nLint (code smells). Can be removed with styler\n\n# code quality assessment\nlint(text = 'x = 1') # with lint\nlint(text = 'x <- 1') # no lint\nlintr::lint_package()"
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#where-things-are-tbc",
    "href": "blog/technotes_20230519_pkgcran/index.html#where-things-are-tbc",
    "title": "R package checklist",
    "section": "Where things are (tbc)",
    "text": "Where things are (tbc)"
  },
  {
    "objectID": "blog/methodnotes_20230601_trialdesign/index.html",
    "href": "blog/methodnotes_20230601_trialdesign/index.html",
    "title": "Clinical trial design",
    "section": "",
    "text": "Coursera course Design and interpretation of clinical trials by Johns Hopkins University"
  },
  {
    "objectID": "blog/methodnotes_20230601_trialdesign/index.html#types-of-trials-designs",
    "href": "blog/methodnotes_20230601_trialdesign/index.html#types-of-trials-designs",
    "title": "Clinical trial design",
    "section": "Types of trials designs",
    "text": "Types of trials designs\nPhase 1: 10-30, identify tolerable dose, information on drug metabolism, extretion and toxicity. Often not controlled\nPhase 2: 30-100, efficacy, safety and side effects,\nPhase 3: 100+, often randomized\nPhase 4: demonstration\n\nTypes of design\nPopulation have the disease outcome of interest; not healthy voluteers vs diseased.\nRandomisation unit: persons, two eyes of a person, or groups of persons\nComparison structure: parallel, crossover, group allocation\n\nParallel: simultaneous treatment and control groups, subjects randomly assigned to one group.\nCrossover: randomize of order in which treatments are received; TC or CT. Each patient is his/her own control. Washout period: time between two treatments.\n\nVariability reduced because less variability within patient than between patients. Fewer patients needed.\nDisadvantages: only certain treatments can use crossover design, treatment can’t have permanent effects. Carry-over effects from first period; washout needs to be long enough. Dropouts more significant, analysis may be more difficult: correlated outcomes.\nConstant intensity of underlying disease: chronic diseases (e.g. asthma, hypertension, arthritis) + short-term treatment effects (relief of signs or symptoms)\ne.g. morning dose vs evening dose\n\nGroup allocation: a group of subjects (community, school, clinic).\n\nExtensions of the parallel design: factorial, large simple\n\nFactorial: two interventions tested simultaneously. Can be presented in a 2 by 2 table (treatment A +-, treatment B +-); or 3 by 2 etc.\n\nInterested in main effect (if no interaction expected). A vs no A; B vs no B. The other treatment doesn’t matter.\n\nLarge simple: large number of patients, possibly from many study sites.\n\n\nTests other than superiority\n\nEquivalency: intervention response is close to control group response\nNon-inferiority: Treatment A (new) is at least as good as B (established). One-sided test, if A is worse than B, one can be rejected. Does not require as big sample size.\n\n\n\nAdaptive design\nPossible adaptations\n\nrandomization probabilities\nsample size (e.g. group sequential methods)\nvisit schedule: shorten/lengthen follow-up time, change number of timing of visits, treatments (dose/duration, concomitant meds)\nhypothesis tested"
  },
  {
    "objectID": "blog/methodnotes_20230601_trialdesign/index.html#randomisation-and-masking",
    "href": "blog/methodnotes_20230601_trialdesign/index.html#randomisation-and-masking",
    "title": "Clinical trial design",
    "section": "Randomisation and masking",
    "text": "Randomisation and masking\nRationale:\n\navoid selection bias: prognostic factors related to treatment assignment\ntends to produce comparable treatment groups\n\n\nSchemes\nSimple randomization, restricted randomization, adaptive randomization\n\nSimple rz\nEach assignment is unpredictable, number of patients in each group should be equal in the long run.\nRisks: imbalances in number assigned to treatment groups, or confounding factors (gender, disease severity) -> reduced power\n\n\nRestricted rz\nSchemes with constraints to produce expected assignment ratio\n\nblocking\nstratification\n\nBlocking. Block of size 2 with treatment allocation ratio 1:1: A,B. Size 4: 2As, 2Bs. Need to be permuted: AABB, ABAB, … in total 6 combinations. Then choose one of the permutations.\nStratification. Ensure balance in treatment assignments with subgroups defined before rz. Limit to a few variables (highly related to outcome and/or logistical): e.g. clinic in a multicenter trial, surgeon (skills, procedures), stage of disease, demographic such as gender and age.\nUse these two together.\n\n\nAdaptive rz\nProbability of assignment does not remain constant, but determined by the current balance and composition of the groups.\n\nminimization: choose the design that gives the smallest imbalance.\nplay the winner: change allocation ratio or favor the better treatment based on the primary outcome. Need to evaluate outcomes relatively quickly.\n\n\n\n\nMasking (blinding)\nTreatment assignment is not known after rz.\n\npatient, clinical personnel, evaluators, data processors, …\nsingle (only participant), double (+ investigator), triple (+ data processors, …), quadruple …\n\nPurpose: remove bias related to treatment effects.\nDifferent levels of masking protects to different extent against bias in different aspects\n\ndata reporting\ndata collection / follow-up\ntesting, behaviors\noutcome assessment\n\nDecision to mask treatments\n\nethical?\npossible? can you make the treatment seem identical so the participants do not know?\ntrial design features: more important to mask subjective ones (e.g. alive or dead is the least subjective, hence wouldn’t benefit much; however if participants need to report effects that are not objectively measureable, they might report that treatment is better in contrast to placebo group)\nfeasible? cost-benefit, practicality (adherence)\n\nSometimes investigators in a double blind study might know which treatment is being assigned to participants, if the effect of drug is very obvious (both good or bad).\nUnmasking\n\nPlanned: inform participants once the trial finished\nUnplanned (discouraged): in the event of adverse event"
  },
  {
    "objectID": "blog/methodnotes_20230601_trialdesign/index.html#outcomes-and-analysis",
    "href": "blog/methodnotes_20230601_trialdesign/index.html#outcomes-and-analysis",
    "title": "Clinical trial design",
    "section": "Outcomes and analysis",
    "text": "Outcomes and analysis\nOutcome: endpoint. It is a quantitaive measure.\nObjectives of the trial\n\nefficacy / effectiveness\nsafety\nprocess\ncosts\n\nExample: evaluate treatment for asthma\nOutcomes: exhaled nitrous oxide, lung function (spirometry measures), asthma symptoms (wheezing, night awakenings), …\nExample: evaluate a procedure to reduce perioperative morbidity\nOutcome considerations: time window (what is postoperative), specific events to be considered an outcome, procedures to establish outcomes, …\n\nMetrics for events as outcomes\n\ndichotomous: 1/0 for presence absense, normal abnormal; clinical state or cut-off value\ntime-to-event: in addition to dichotomous, add time dimension; allow for censoring. More powerful than dichotomous.\nrates: 1/0 but allow for repeats, analyze count or rate. Events within a person are usually not independent, need to account for it.\ncontinuous variables: value or change from baseline; standard units (lab values, scores). Need to define an important difference. Distributional assumptions more important.\nordinal scale: ranked categories (e.g. adverse event grading, 1-5). Difference between categories is usually qualitative.\n\nPatients opinions are subjective\n\nhealth status / change in status, e.g. pain relief, quality of life\nmasking is more important\nhawthorne / placebo effect: effect of being studies, usually positive\nquantify with standardized scales\n\n\n\nInfluence of outcomes on design\nEfficacy vs effectiveness:\nIn a vaccine trial, efficacy is the clinical case with lab confirmation; effectivenenss is the clinical case of influenza in a larger population, may or may not be confirmed.\nIn asthma, efficacy is FEV1, effectiveness is the decrease of the hospitalizations/steroid courses.\nConsiderations (3Bs)\n\nbiology: does outcome reflect a clinically relevant fact/change\nbiostatistics: detectable difference between groups is plausible and practical\nbudget: afford total N and can measure it reliably in every participant\n\nExample: HIV trial outcomes\n\nsurvival (deaths; AIDS status)\nimmunologic response\nvirologic response\nchange in patient status (e.g QoL)\nspecified toxicity\nother side effects\n\nChoice of primary outcome depends on the objectives or stage of research\n\nphase 1, emphasis on safety\nphase 2, short-term efficacy\nphase 3, long-term efficacy\nphase 4, long-term effectiveness\n\n\n\nIntention to treat ITT\nCross-overs after rz: some patients might have a treatment (yes or no) beyond what they were assigned to, e.g. refuse surgery or medical treatment\nNon-adherence during followup: some in treatment group refuse of can not tolerate certain treatment; while some in placebo group require medication or take on their own\n\n\nSubgroup analysis\nStratified analysis: estimate treatment effect separately in subgroups. Does not tell difference across different subgroups\nTest for interaction: use of main effect and interaction.\nIssue of multiple testing when doing a series of analyses\n\ninflate sample size to plan for subgroup analysis\nreport number of subgroup analyses performed\npossibly adjust for multiple comparisons\nreport CI instead of just p-values"
  },
  {
    "objectID": "blog/methodnotes_20230601_trialdesign/index.html#ethics",
    "href": "blog/methodnotes_20230601_trialdesign/index.html#ethics",
    "title": "Clinical trial design",
    "section": "Ethics",
    "text": "Ethics"
  },
  {
    "objectID": "blog/methodnotes_20230601_trialdesign/index.html#reporting-results",
    "href": "blog/methodnotes_20230601_trialdesign/index.html#reporting-results",
    "title": "Clinical trial design",
    "section": "Reporting results",
    "text": "Reporting results\n\nCONSORT\nConsolidated Standards of Reporting Trials\n25 item checklist. Not a guideline of how to do the trial; but a guideline for writers on how to report clearly.\nTitle\n\nkey design items\ntreatment(s) evaluated\ndisease or population studied\n\nAbstract\n\ndesign, method, results, conclusions\n\nIntroduction\n\nbackgrounds, rationale, establish equipoise (balance), systematic review, objectives / hypothesis\n\nMethod\n\nIRB review and approvals\ntrial design, allocation ratio\neligibility criteria\nsetting and location of the trial\nintervention - detailed enough to allow replication\noutcomes - primary, secondary, how assessed and defined\nsample size - how determined, interim analyses\nimportant changes during trial\nrandomization, allocation concealment, implementation\nmasking - who, how\nstatistical methods - primary and secondary; subgroup analyses\n\nConvenient to use a flow-chart\nBaseline characteristics by treatment group\np-values (common to not report in rct?)\n\n\nEvaluate literature\n\nLegitimacy, is it a fair comparison\nTrustworthy investigators? conflict of interest\nAdequate protections against bias\n\nrandomization\nmasking\nfollow-up design and execution\n\nITT analysis\n\nhave all events (outcomes) observed been counted in the treatment group assigned?\nvariations in denominators explained and consistent with good practice?\n\nappropriate subgroup analysis interpretation\n\nad hoc or post hoc status"
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#build-package-and-check",
    "href": "blog/technotes_20230519_pkgcran/index.html#build-package-and-check",
    "title": "R package workflow",
    "section": "Build package and check",
    "text": "Build package and check\nIt is possible that your checks don’t pass on the first try.\n\nWhat to ignore when build?\n^.*\\.Rproj$\n^\\.Rproj\\.user$\n^dev$\n^_pkgdown\\.yml$\n^license\\.md$\nMakefile\ndata-raw\ncran-comments.md\n^\\.github$"
  },
  {
    "objectID": "blog/blog_20230717_teaching/index.html",
    "href": "blog/blog_20230717_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Earlier this year (2023) I wrote a blog about my thoughts on the role of open source software in statisical education. Naturally, I advocate for more use of open source tools such as R/python in teaching introductory statistics to applied scientists. Nonetheless, how the material is taught will make a huge difference in the understanding and interest in the material.\nI was taught statistics in the classic way: lectures with tons of mathematical formulae and proofs, while programming and data analyses were left for students themselves to figure out. Those who were the fastest learners were the ones who already had a degree in computer science, which probably doesn’t sound surprising. I, for one, definitely struggled."
  },
  {
    "objectID": "blog/blog_20230717_teaching/index.html#does-statistics-have-to-be-daunting",
    "href": "blog/blog_20230717_teaching/index.html#does-statistics-have-to-be-daunting",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "Does statistics have to be daunting?",
    "text": "Does statistics have to be daunting?\nFor applied scientists in various fields, data analysis is a core task, and also a challenging one. You must have met clinicians or biologists who would love their data to be analysed yet don’t know how to. Yes, statistics and data skills can take some time to learn; but with the right method, they don’t have to be daunting. It is up to the educator to find a way that benefits the most students. An observation is that many researchers do not know or remember advanced math; yet do they need advanced math to grasp many fundamental statistical concepts?\nI believe that it is far more important and useful to teach basic IT skills and exploratory data analysis so that students can develop an understanding of their own data; rather than using a test blindly."
  },
  {
    "objectID": "blog/blog_20230717_teaching/index.html#rebooting-mf9130e-classroom",
    "href": "blog/blog_20230717_teaching/index.html#rebooting-mf9130e-classroom",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "Rebooting MF9130E classroom",
    "text": "Rebooting MF9130E classroom\nWhen I heard that the teaching team at Biostatistics Department, Faculty of Medicine was thinking about trying a novel pedagogical method on the MF9130E (2023 spring) class, I was more than excited to contribute. This is a PhD level course of 8 days long, offered three times a year (twice in Norwegian language). Students come from a variey of backgrounds in health and life sciences. Since this is an introductory course, the topics are broad rather than specialised.\nA few years ago, statistical software for the course made the transition from SPSS to Stata. To be more precise, students were introduced to, but not really explained to, or elaborated on how to use Stata proficiently. Why? The course is about statistics so only statistics is taught. Data skills such as manipulation are not part of statistics. \nWell, we will change that by starting to use R.\n\nThree open source musketeers\nR, quarto and GitHub the three musketeers in facilitating the transformation. We build a quarto course website where all the material are public, hosted with GitHub Pages. Having a course website is beneficial for students to have an overview of the course, in contrast to many scattered lecture notes and exercises to be downloaded.\nThe biggest advantage of using quarto is the rendered output from code. From a student’s perspective, it is reassuring to see the same result and plots using the data and code provided by the instructor. For the instructor, it is also convenient to see whether the code functions as expected. When we do not want to show the output, it is also very easy to suppress. We have created one copy with and one withtout rendered output as exercises, and are glad to see some students challenging themselves by attempting to solve the problems without solution.\nUsing Github and quarto together to build a course website is rather straightforward. I think the site structure is simple yet flexible enough to navigate. Collaboration across a small teaching team is also manageable. Github Pages was easy to set up, and changes made on the main branch is deployed within the minute. This proved to be useful in quite a few moments (where we had to replace some datasets or add some notice).\n\n\nThe Carpentries pedagogical model\nThe Carpentries is an organisation that teaches foundational coding and data science skills to researchers. I myself benefited from their workshop on version control and git taught at University of Oslo, and I think the traditional classroom could use some of the methods at these data science workshops.\nTo put simply, there are two things I tried with the course setup for MF9130E:\n\nLive coding demonstration, plenty of it\nSticky-notes flag and helper (teaching assistant) in class\n\nIn the live coding demonstration (which I was responsible for), I made sure that students were taught the most commonly used R commands for data manipulation and exploration. Quarto webpages on introduction to R, basic EDA, intermediate EDA have been created and guided through in class, mixed with statistical concepts and visualizations. Without knowing how your data looks like, blindly using statistical tests is dangerous - that is the motivation for doing so.\nWhether students feel supported can make a huge difference in their willingness to learn. Taking it slow at the beginning, and solve the problems on an individual basis can prevent early drop-outs, especially when programming and IT systems are involved. Naturally, when we don’t have helpers we can not help everyone; this is a limitation for this model. Students should be encouraged to help each other.\n\n\nLet them explore\nThe last important change in the class was to give time to students themselves. We reduced the lecturing on theory and computation, and added time for practice and discussion. The guided practice with live demo also came with solution and comments, so students could explore at their own pace. We left plenty of time for them to ask questions, and made sure most people can follow the exercises."
  },
  {
    "objectID": "blog/blog_20230717_teaching/index.html#how-did-it-go",
    "href": "blog/blog_20230717_teaching/index.html#how-did-it-go",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "How did it go?",
    "text": "How did it go?\nAfter the 8 day course we carried out a small survey among the ~50 students in the spring 2023 class. Student backgrounds are diverse, they work on lab data, clinical data or observational/epidemiological data:\n\nobservational study on humans 36%\nRCT 18%\nin vitro research 15%\nothers are in animal research, meta analysis or something else\n\nStatistical competency (method, software) among students are generally on the basic end. Over 75% of the cohort report themselves to have basic to very basic knowledge of statistics; 33% do not use any statistical software, around 45% have used SPSS or Stata. On the other hand, some students (7%) report to have advanced knowledge and have some R experience.\n\nSome feedback\nThis is the first time we do the course with R, live demo and put an emphasis on basic data manipulation and exploration - which means we do not have enough data, it is just an initial impression.\nHere’s what we have received. On the positive side, 86% find the course useful for their own PhD research. 75% felt they are able to use the correct methods for their analyses, which is quite encouraging. Most felt the examples and exercises were able to demonstrate the theory. Students have generally positive experience with the live demo, and find the instructors supportive. This is good!\nIn the meantime, it is only natural that some are dissatisfied (21%) in some ways. Common complaints are: R is not user friendly to absolute beginners; the leap from no software to a programming language is too big for some.\nAs for whether students have really mastered the knowledge intended, we do not have enough data to draw a conclusion. We do observe that the take home project show somewhat better understanding, but can not say for sure just yet.\nThis is a class with very diverse backgrounds, hence it is challenging to cater to everyone’s needs. Yet, we are satisfied with the trial-transformation with our introductory statistics class, and we plan to gradually implement more classes with R, and possibly hands-on practice (depending on capacity)."
  },
  {
    "objectID": "talks/talks.html#public-health",
    "href": "talks/talks.html#public-health",
    "title": "Talks",
    "section": "Public Health",
    "text": "Public Health\n\n\n\n\n\n\n\n\n\n\nIntroducing Sykdomspulsen\n\n\nAn automated public health surveillance platform\n\n\n45 min talk and tutorial at Oslo UseR meetup \n\n\n\n\n\nJun 16, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/rstats_20230721_teaching/index.html",
    "href": "talks/rstats_20230721_teaching/index.html",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "",
    "text": "Time and place: July 21, 2023 10AM. Roche office, Basel, Switzerland\nSlides for this talk can be accessed here."
  },
  {
    "objectID": "talks/rstats_20230721_teaching/index.html#about-the-topic",
    "href": "talks/rstats_20230721_teaching/index.html#about-the-topic",
    "title": "Transforming medical statistics classroom with R and Quarto",
    "section": "About the topic",
    "text": "About the topic\nThe 8 day introductory statistics course (MF9130) at the Faculty of Medicine, University of Oslo is designed for PhD students in medicine, biology, psychology and other health related fields. Similar to other conventional teaching methods, the course has been focusing largely on theory and hand calculation. The software has been Stata and SPSS, and data analysis was mostly left for the students to figure out on their own.\nThis year, we made an attempt to transform the course with R, and aimed to teach more practical data analysis skills. We added one session per day where the instructor guide students on R and project management, importing data , basic manipulation and statistical methods. The IT skills of the students vary greatly, and therefore we used the ‘sticky notes’ help system borrowed from the Carpentries to make sure everyone could get help in the first days. We have created a course website using Quarto, where all the material and R exercises (with rendered solution) are available for self-study. We have witnessed amazing progress - by the end of the first week, students with the least computer / data skills were able to work on dataframes, make basic plots and do a chi-squared test. This helps build students confidence in data and statistics, and as a result, they can start to work on their own datasets using the skills immediately."
  },
  {
    "objectID": "blog/blog_20230904_cen2023/index.html",
    "href": "blog/blog_20230904_cen2023/index.html",
    "title": "Personal Highlights: CEN2023",
    "section": "",
    "text": "The IBS (International Biometric Society) conference of the Central European Network, CEN2023 has been a great opportunity to keep myself up to date with the latest development of biostatistics, both in academia and industry. Thanks to the great effort made by the organizing committee and almighty Google Meet/Zoom, I have been able to follow the talks without any issue, and have definitely learned a lot.\nGiven my background, I paid more attention on talks and workshops on\n\nStatistical software, R programming and simulation\nCausal inference\n\nThere were also two topics that drew my attention: one is on statistical education towards medial professionals, the other is on a Data Challenge using RCT data.\n\nStatistical software\n\nSoftware Engineering Working Group (SWE WG), MMRM\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nDaniel Sabanes Bove (Roche). First year of the Software Engineering working group - working together across organizations\nGonzalo Duran-Pacheco (Roche). Comparing R libraries with SAS’s PROC MIXED for the analysis of longitudinal continuous endpoints using MMRM\n\n\n\n\nThe ASA Biopharmaceutical Section (BIOP) Software Engineering Working Group SWE WG was established in 2022. Currently they have 3 work streams:\n\nmmrm implements MMRM (mixed models with repeated measures)\nbrms.mmrm, the Bayesian version of MMRM\nHealth Technology Assessment with R\n\nAt a later talk, mmrm was compared with SAS’s PROC MIXED and R’s nlme, glmmTMB for analyses of longitudinal continuous endpoints. In terms of speed and convergence, mmrm is superior than others; while the estimate prodouced by mmrm is very close to PROC MIXED and glmmTMB.\nThis looks like a very interesting tool to try out! Vignette\n\n\nSimulation tools and RWD\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nMichael Kammer (Medical University of Vienna). An overview of R software tools to support simulation studies: towards standardizing coding practices.\n\n\n\n\nKammer and colleagues did a review on R packages for simulation, and selected 14 top simulation packages, including simstudy, simdata, synthpop, bigsimr and others. The full list is made available here.\nA real-world dataset, NHANES was also introduced here. The data can be accessed with R package nhanesA.\n\n\n\nCausal Inference\n\n\n\n\n\n\nInformation\n\n\n\n\n\nWorkshop: Implementing the estimand framework in global drug development: Application of causal inference approaches (Mouna Akacha, Björn Bornkamp, Alex Ocampo, Jiawei Wei at Novartis)\nKeynote: Ruth Keogh (LSHTM). Causal inference with observational data: A survival guide\n\n\n\nThese two workshop / talk cover slightly different scenarios: one in RCT, one for observational data. It deserves a whole article or more to elaborate on this topic, so I’m only putting some resources here.\nCausal inference is definitely gaining traction in recent years in both academia and industry. Techniques such as g-computation, IPW and doubly robust estimation are starting to become mainstream. It is fascinating that these techniques themselves are not bound to a fixed model.\nResources:\n\nWorkshop repository Causal-inference-in-RCTs\nBook: Causal Inference: What If by Hernán and Robins (2020)\nPrincipal stratum strategy, Bornkamp et al. (2021)\nTime-dependent covariates, Keogh et al. (2023)\nTarget Trial Emulation (TTE), Hernán and Robins (2016)\n\n\n\nCovariate adjustment and data challenge with RCT data\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nKelly Van Lancker (Ghent University). Improving Power in Randomized Trials by Leveraging Baseline Variables\nDominic Magirr (Novartis). Organizing a Data Challenge on Covariate Adjustment in RCTs\nCraig Wang (Novartis). Participating in a Data Challenge on Covariate Adjustment in RCTs\n\nPanel discussion: Jonathan Bartlett (LSHTM)\n\n\n\n23 teams at Novartis participated in a Data Challenge on Covariate Adjustment. They were given a fixed outcome model, and 5 prior studies trial data, and their task was to create the design matrices that improve the precision compared to unadjusted data.\nIf I were to select talks based on the category titles, I would probably missed the whole session. However, it is surprisingly similar to using not trial, but real-world data (such as EHR) to make predictions. The conclusion were similar as well: using “supercovariates” created by ML isn’t gaining much compared to simple models such as ANCOVA. Possible reasons:\n\nsmall to moderate data size\nlinear relationship between covariates and outcome\ngood enough prognostic variables\n\nIt was also mentioned that the winning team did some trick to reduce the variance among the covariates. Would be interesting to read about it.\nSome resources:\n\nLancker et al. The use of covariate adjustment in randomized controlled trials: an overview link\nCovariate adjustment tutorial, link\n\n\n\nStatistical education\n\n\n\n\n\n\nTalk information\n\n\n\n\n\n\nMaren Vens et al (University of Lübeck). Biostatistics/Biometrics for physicians – essential or unnecessary? How do practicing physicians and dentists evaluate biostatistics? A cross-sectional survey\n\n\n\n\nStatistical education to students / professionals who are not used to working with data has always been tricky. Students generally think statistics is difficult, and need help from a statistician. However there are only limited number of statisticians. The talk by Vens and colleagues confirms what practicing statisticians know, but can’t do much about: most (87%) physicians and dentists in the survey need a statistician to help with their work.\nHow to improve the statistical competency is an important and relevant topic for discussion, and might require systematic changes in how it is taught. Use of modern technology can help, yet it’s only helpful when students start to not fear, or not find math and technology boring."
  },
  {
    "objectID": "blog/blog_20230904_cen2023/index.html#software-engineering-working-group-swe-wg-mmrm",
    "href": "blog/blog_20230904_cen2023/index.html#software-engineering-working-group-swe-wg-mmrm",
    "title": "Personal Highlights: CEN2023",
    "section": "Software Engineering Working Group (SWE WG), MMRM",
    "text": "Software Engineering Working Group (SWE WG), MMRM\nThe ASA Biopharmaceutical Section (BIOP) Software Engineering Working Group SWE WG was established in 2022. Currently they have 3 work streams:\n\nmmrm implements MMRM (mixed models with repeated measures)\nbrms.mmrm, the Bayesian version of MMRM\nHealth Technology Assessment with R\n\nAt a later talk, mmrm was compared with SAS’s PROC MIXED and R’s nlme, glmmTMB for analyses of longitudinal continuous endpoints. In terms of speed and convergence, mmrm is superior than others; while the estimate prodouced by mmrm is very close to PROC MIXED and glmmTMB.\nThis looks like a very interesting tool to try out! Vignette"
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html",
    "href": "blog/blog_20230921_positconf2023/index.html",
    "title": "Personal Highlights: Positconf 2023",
    "section": "",
    "text": "The yearly party of Positconf (formerly Rstudio conf) has come to an end. I joined the virtual experience at home, it is of course not the same as attending in-person, yet the atmosphere in discord was still great!\nIt’s hard to choose which talks to watch since multiple were scheduled at the same time, so one has to prioritize. I definitely will re-visit some of the talks at a later point, so this blog acts as a placeholder for links so that I can find them in the future."
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html#make-interactive-things",
    "href": "blog/blog_20230921_positconf2023/index.html#make-interactive-things",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Make interactive things",
    "text": "Make interactive things\nWebDev is definitely a big thing at this year’s positconf. If I’m learning one thing from the conference, I’d check out webR.\nI still remember when R was mainly for statistical analysis and computing back when I learned it. Now it’s become much more fun! Strictly speaking, webRand quarto are not R per se. However, they’ve become the gateway drugs for R programmers to dabble in WebDev. With web assembly (wasm), now one can execute R code in a browser and even run shiny app.\nUnlock the power of dataViz animation and interactivity in quarto by Deepsha Menghani used a super fun example (F-bomb) to demonstrate how to add interactivity to your barplot (or other plots) with Crosstalk. Check out the talk here. The presentation was as interactive as the quarto slides, good job Deepsha!\nRunning shiny without a server by Joe Cheng (repo): this was a big announcement. I used shiny at work, but for my own projects or smaller teaching projects I tried to stay away from shiny - I was concerned about the fee. This looks like a promising thing to try out once it’s stable, although I’d probably do webR first."
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html#quarto-updates",
    "href": "blog/blog_20230921_positconf2023/index.html#quarto-updates",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Quarto updates",
    "text": "Quarto updates\nQuarto is definitely one of the most discussed topics in the year 2022-2023 in the R community. For good reasons. I need to catch up the the latest developments annd use-cases:\n\nWhat’s new in quarto? by Charlotte Wickham\nReproducible manuscripts with Quarto by Mine Çetinkaya-Rundel\nParametrized quarto reports improves understanding of soil health by Jadey Ryan\n\nand so many more. I couldn’t follow all the talks and I’m sure there are lots of great examples of how quarto is better than traditional ways of reporting."
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html#other-things-to-check-out",
    "href": "blog/blog_20230921_positconf2023/index.html#other-things-to-check-out",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Other things to check out",
    "text": "Other things to check out\ntargets package for pipeline automation and management"
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html#make-pretty-things",
    "href": "blog/blog_20230921_positconf2023/index.html#make-pretty-things",
    "title": "Personal Highlights: Positconf 2023",
    "section": "Make pretty things",
    "text": "Make pretty things\nIt is fascinating to see so many organizations and individual R developers make their own themes for better branding, recognition and storytelling. More and more peple have realized that making beautiful plots is important, and totally possible as well. Work on layout, color, font and sizes!\n\nThemes\nAdding a touch of glitr: Developing a package of themes on top of ggplot by Aaron Chafetz, Karishma Srikanth and colleagues at USAID. repo\n\n\nTables\nMaking tables with gt has been on my to-do list for a while now. It is very inspiring to see so many cool tables that makes you wonder, “is it really JUST a table?” For example, check out this gallery by Posit community.\nThe book Creating beautiful tables in R with gt by Albert Rapp would be a good place to learn how to make nice tables. Actually the reason why I wanted to use gt is that it seems to be the mainsteam in clinical reporting in pharma. I bumped into this blog post some time ago, and this would be my starting point.\n\n\nQuarto\nIf you want to go one step further and start making your quarto project pretty, there are a few things to try out.\nAlbert Rapp in his talk HTML and CSS for R Users stated that quarto is a gateway drug to WebDev. It reminds me of my very first presentation at my local R users community (2019) was about building a website with blogdown, and when I really spent a lot of time to make my markdown documentation colorful with span style - and that was about everything I knew.\nNow I want more. Learning HTML and CSS can make your dataviz, tables, slides and dashboards look not only professional but also special. I’m going to check out the scss variables in quarto which defines the theme, theme_file.scss. Emil Hvitfeldt (Styling and templating quarto documents) showed us how to make really pretty and animated (!) quarto sldies themes, and shared this template with us, quarto-revealjs-earth. I really like how revealjs slides look like, just that the MacOS Keynote (or MS ppt) drag-and-drop seems more flexible to me (?) Guess it’s something I should get used to over time.\nRichard Iannone (Extending quarto) introduced quarto shortcode extensions to add a bunch of fancy-looking icons to quarto files. To create extensions in general: https://quarto.org/docs/extensions/creating. This is for more pro-users since you needs to learn lua."
  },
  {
    "objectID": "blog/blog_20230921_positconf2023/index.html#a-few-other-things-to-check-out",
    "href": "blog/blog_20230921_positconf2023/index.html#a-few-other-things-to-check-out",
    "title": "Personal Highlights: Positconf 2023",
    "section": "A few other things to check out",
    "text": "A few other things to check out\nBeyond the web and quarto topics, I think there are some existing and new tools that can be useful for my work. For example,\n\nI should review Hadley and Jenny’s R package book (2e).\nthis package targets for pipeline automation and management look like something that can be used for my analysis\n…\n\nIt will take a while to digest the latest developments. But little by little, we’ll get there! People in the R community are doing great things."
  },
  {
    "objectID": "blog/technotes_20231001_qt_webr/index.html",
    "href": "blog/technotes_20231001_qt_webr/index.html",
    "title": "Use WebR in your existing quarto website",
    "section": "",
    "text": "WebR is the new hot topic in the R community. Coupled with Quarto, you can run R code interactively in a web browser. This is achieved with the great quarto extension, quarto-webr developed by James J Balamuta.\nIn the positconf 2023 talk, documentation and YouTube, James introduced how to make a webR empowered quarto document. It is simple enough, and you can make it work quite smoothly."
  },
  {
    "objectID": "blog/technotes_20231001_qt_webr/index.html#when-your-render-gets-stuck",
    "href": "blog/technotes_20231001_qt_webr/index.html#when-your-render-gets-stuck",
    "title": "Use WebR in your existing quarto website",
    "section": "When your render gets stuck",
    "text": "When your render gets stuck\nBut there is a twist. This works perfectly fine with a new quarto project, where no output-dir is specified yet. When I tried to replicate the same thing for my existing quarto website (with output-dir: docs so that I could deploy it with GitHub Pages), my rendered html file got stuck:\n\nIf you read the troubleshooting documentation, you’ll see that it’s a problem with the two js files. This agrees with what Rstudio Background Jobs tells us.\n\nI moved the two files (manually..) around, then render again, nothing changed.\n\nSolution: set channel-type option\nThis is a solution provided by the authors, although I don’t quite understand what it did, but it did the magic. (Thanks to Linh’s help!)\nThis is where you specify this option.\n\nRender again, now it works! WebR status turns green, and I can run code interactively in the browser."
  },
  {
    "objectID": "blog/technotes_20231001_qt_webr/index.html#summarize-the-workflow",
    "href": "blog/technotes_20231001_qt_webr/index.html#summarize-the-workflow",
    "title": "Use WebR in your existing quarto website",
    "section": "Summarize the workflow",
    "text": "Summarize the workflow\nThe workflow to add the quarto-webR extension to your existing quarto website is almost identical as adding it to a new project:\n\n1. Install\nInstall the extension by running this line in the terminal (for the current project)\nquarto add coatless/quarto-webr\n### 2. Configure\nConfigure the YAML header for your_demo.qmd\ntitle: \"Your demo\"\nengine: knitr\nformat: html\nfilters: \n  - webr\nwebr: \n  channel-type: \"post-message\"\nImportant bits:\n\nspecify engine to knitr\nspecify filters to - webr. This could alternatively be specified in the overall _quarto.yml file to apply to every qt document.\nadd channel-type: \"post-message\" under webr. No dash in front.\n\n\n\n3. Execute\nNow use the curly bracket {webr-r} for your code chunk (which used to be just {r}),\nLoading\n  webR...\n\n\n  \n\n\nA histogram that changes every time you click RUN CODE. This proves that we are running interactively the R code inside the web browser.\nLoading\n  webR..."
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#initialize-the-project",
    "href": "blog/technotes_20230519_pkgcran/index.html#initialize-the-project",
    "title": "R package workflow",
    "section": "Initialize the project",
    "text": "Initialize the project\n\nusethis::create_package('path_to_pkg/pkgname') \n\nIt opens a new R project (directory) named pkgname, with the following items:\n\nDESCRIPTION\nNAMESPACE\ndirectory R/\n.Rbuildignore and .gitignore\nand the project icon, pkgname.Rproj.\n\nIf you have an existing R project but wish to build a package there, copy everything but pkgname.Rproj, and modify the files in your existing pkg directory. Pay extra attention to the hidden files like .Rbuildignore.\n\nusethis::use_mit_license() # modify name to yours\nusethis::use_readme_md() # if you do not have this already\nusethis::use_news_md()\nusethis::use_test()\n\n# create a folder for future data documentation\nx &lt;- 1 \nusethis::use_data() \n\nIn addition, URL and bug reports should be added in the DESCRIPTION."
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#planning",
    "href": "blog/technotes_20230519_pkgcran/index.html#planning",
    "title": "R package workflow",
    "section": "Planning",
    "text": "Planning\nIt is good practice to start with planning the package, rather than directly start coding.\nCreate a folder called dev. To prevent it from being built, add the following line in .Rbuildignore"
  },
  {
    "objectID": "blog/technotes_20230519_pkgcran/index.html#write-test-and-document",
    "href": "blog/technotes_20230519_pkgcran/index.html#write-test-and-document",
    "title": "R package workflow",
    "section": "Write, test and document",
    "text": "Write, test and document\nCreate exported functions in R/, development code in script/ (or somewhere else, such as dev/).\n\nData: raw and processed\nNeed to be clear in mind where the data files go. There are a few data related folders:\n\nraw data files, in the format of excel sheets or csv. Usually placed as inst/data_name.csv\nR scripts to process the raw data so that we create data object inside the package, put inside data-raw\ndata objects that can be called as pkg::data_name, are placed in data. These files are usually directly generated by executing write.rda().\ndata documentation, usually placed in R/data_documentation.R. These are Roxygen2 documents for the data.\n\n\n\nDocumentation\nYou need to configure the Build tools.\nThese three things should be done:\n\nFunction documentation\nCreate a function f1, and put your cursor on it. Go to Code -&gt; Insert Roxygen Skeleton to create the template.\nAlternatively, use #' to start.\n\n#' A simple placehold function \n#'\n#' @param x a numeric value\n#'\n#' @return a value 3 greater than the input\n#' @export\n#'\n#' @examples \n#' f1(5)\nf1 &lt;- function(x){\n  x+3\n}\n\n\n\nData documentation\nIt can be beneficial to create a separate file to document data only, say data_documentation.R under the R/ directory.\n\n#' Placeholder data x\n#'\n#' This dataset contains one value, x\n#'\n#' @format\n#' \\describe{\n#' \\item{x}{The placeholder data x}\n#' }\n#' @examples\n#' print(x)\n\"x\"\n\n\n\nVignette documentation\n\nusethis::use_vignette('your_vignette')\n\n\n\nDeploy to pkgdown\nCheck this reference here"
  },
  {
    "objectID": "blog/technotes_20231018_qt_styling/index.html",
    "href": "blog/technotes_20231018_qt_styling/index.html",
    "title": "Styling your quarto project",
    "section": "",
    "text": "Useful references:\n\nTalk by Emil Hvitfeldt on Styling and Templating Quarto Documents"
  },
  {
    "objectID": "talks/talks.html#rstats-and-quarto",
    "href": "talks/talks.html#rstats-and-quarto",
    "title": "Talks",
    "section": "Rstats and Quarto",
    "text": "Rstats and Quarto\n\n\n\n\n\n\n\n\n\n\nA one year recap on teaching statistcis to medical students: how can R and Quarto help?\n\n\nR/Medicine 2024\n\n\n\n\n\n\n\n\nJun 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTransforming medical statistics classroom with R and Quarto\n\n\nBasel R meeting\n\n\nJoin us to hear about how we ran a trial classroom (MF9130E) with R and Quarto at the Faculty of Medicine, University of Oslo\n\n\n\n\n\nJul 21, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "teaching/teaching.html#introductory-statistics",
    "href": "teaching/teaching.html#introductory-statistics",
    "title": "Teaching",
    "section": "Introductory Statistics",
    "text": "Introductory Statistics\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nERN4110\n\n\nStatistics for master students in nutrition\n\n\n\n\nMF9130/MF9130E\n\n\nIntroductory course in statistics\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html#create-a-public-repository-on-github",
    "href": "blog/technotes_20230111_deployqt/index.html#create-a-public-repository-on-github",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "",
    "text": "After you have the public repo, clone it to your local repo."
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html#create-quarto-project",
    "href": "blog/technotes_20230111_deployqt/index.html#create-quarto-project",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "2. Create Quarto project",
    "text": "2. Create Quarto project\nThis can be a website, a book (a specific type of website) or something else.\nTest compilation by quarto render, or click the Render button."
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html#configure-quarto-project",
    "href": "blog/technotes_20230111_deployqt/index.html#configure-quarto-project",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "3. Configure Quarto project",
    "text": "3. Configure Quarto project\nIn _quarto.yml, change the project configuration to use docs as the output-dir:\nproject:\n  type: website\n  output-dir: docs\n\n\n\n\n\nThen add .nojekyll to the root of the repository. Can do this by (in terminal)\ntouch .nojekyll\nPush everything to your repository."
  },
  {
    "objectID": "blog/technotes_20230111_deployqt/index.html#configure-github-pages",
    "href": "blog/technotes_20230111_deployqt/index.html#configure-github-pages",
    "title": "Publishing Quarto Website with GitHub Pages",
    "section": "4. Configure GitHub Pages",
    "text": "4. Configure GitHub Pages\nGo to Settings &gt; Pages, publish from docs of the main branch.\n\n\n\n\n\nCan check GitHub Action and deployment status.\n\n\n\n\n\n\n\n\n\n\nAfter the deployment is successful, go to view deployment, and a successful website should be published."
  },
  {
    "objectID": "rpkg/rpkg.html#public-health",
    "href": "rpkg/rpkg.html#public-health",
    "title": "R packages",
    "section": "",
    "text": "Title\n\n\nDescription\n\n\n\n\n\n\ncovidnor\n\n\nOpen COVID19 data for Norway\n\n\n\n\ncsalert\n\n\nOutbreak detection in public health surveillance\n\n\n\n\ncsdata\n\n\nPre-formatted structural data for Norway\n\n\n\n\ncsmaps\n\n\nPre-formatted map data in Norway\n\n\n\n\ncstime\n\n\nDate and time functions for public health purposes\n\n\n\n\nnoreden\n\n\nTools to facilitate nutrition researcher for sustainable diet discovery\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "rpkg/rpkg.html#medicine",
    "href": "rpkg/rpkg.html#medicine",
    "title": "R packages",
    "section": "Medicine",
    "text": "Medicine\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nbayesynergy\n\n\n(Contributor) Synergistic interaction effects in in vitro drug combination…\n\n\n\n\nmedicode\n\n\nMetadata and tools for medical classification and clinical coding\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "rpkg/rpkg.html#quarto-tools",
    "href": "rpkg/rpkg.html#quarto-tools",
    "title": "R packages",
    "section": "Quarto Tools",
    "text": "Quarto Tools\n\n\n\n\n\n\n\n\n\n\nqtwAcademic\n\n\nQuarto Website Templates for Academics\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "rpkg/noreden/index.html",
    "href": "rpkg/noreden/index.html",
    "title": "noreden",
    "section": "",
    "text": "This package provides tools to facilitate sustainable diet discovery.\nGitHub Link"
  },
  {
    "objectID": "teaching/workshop_rladies_zurich/index.html",
    "href": "teaching/workshop_rladies_zurich/index.html",
    "title": "Zero to Quarto Workshop",
    "section": "",
    "text": "Date: Monday 23 October 2023, 6:30PM to 8:00PM, CEST\nPlace: KO2-F-153, Rämistrasse 71, Zürich.\nEvent link\nPresenter: Chi Zhang. Feel free to reach out with comments and feedback!\nWorkshop site"
  },
  {
    "objectID": "projects/noreden/index.html",
    "href": "projects/noreden/index.html",
    "title": "Noreden",
    "section": "",
    "text": "Noreden"
  },
  {
    "objectID": "projects/dan/index.html",
    "href": "projects/dan/index.html",
    "title": "Data Apothecary’s Notes",
    "section": "",
    "text": "(This is my own note-taking system using quarto)\n\nAbout the notes\nData Apothecary’s Notes is a note-taking repository for modern data science skills with a focus on drug development and clinical trials. Content will be gradually added while I learn the topics. Therefore, it is by no means a complete guide by the time you read it!\nI try to organize the content in a modular way. I think these should cover the important aspects in which a data scientist / modern statistician should know.\n\nstudy design\ninference\nmodels\nreporting\nprogramming\n\n\n\nWhy quarto\nIn short, quarto has the advantage of making a very well structured website with code chunks easy. No more worry about scattered notes in different folders - put them together, render it so you can find your notes quickly!"
  },
  {
    "objectID": "projects/phuse/index.html",
    "href": "projects/phuse/index.html",
    "title": "PHUSE",
    "section": "",
    "text": "I am contributing to two working groups at PHUSE: CAMIS, and RWD - Real World Data Guideline (early stage).\n\nCAMIS: Comparing Analysis Method Implementations in Software\nCAMIS is a cross-industry group formed of members from PHUSE, PSI and ASA.\nSeveral discrepancies have been discovered in statistical analysis results between different programming languages, even in fully qualified statistical computing environments. Observing differences across languages can reduce the analyst’s confidence in reliability and, by understanding the source of any discrepancies, one can reinstate confidence in reliability.\nThe goal of this project is to demystify conflict when doing QC and to help ease the transitions to new languages and techniques with comparison and comprehensive explanations.\nI am the content curation co-lead in the project, and contribute to topics under non-parametric tests.\n\n\nRWD Working Group\nThis is a newly formed working group, working on statistical programming guidelines while working on RWD (read world data).\nPlanning stage"
  },
  {
    "objectID": "projects/phuse/index.html#camis-comparing-analysis-method-implementations-in-software",
    "href": "projects/phuse/index.html#camis-comparing-analysis-method-implementations-in-software",
    "title": "PHUSE",
    "section": "CAMIS: Comparing Analysis Method Implementations in Software",
    "text": "CAMIS: Comparing Analysis Method Implementations in Software\nCAMIS is a cross-industry group formed of members from PHUSE, PSI and ASA.\nSeveral discrepancies have been discovered in statistical analysis results between different programming languages, even in fully qualified statistical computing environments. Observing differences across languages can reduce the analyst’s confidence in reliability and, by understanding the source of any discrepancies, one can reinstate confidence in reliability.\nThe goal of this project is to demystify conflict when doing QC and to help ease the transitions to new languages and techniques with comparison and comprehensive explanations.\nI am the content curation co-lead in the project, and contribute to topics under non-parametric tests."
  },
  {
    "objectID": "projects/phuse/index.html#rwd-working-group",
    "href": "projects/phuse/index.html#rwd-working-group",
    "title": "PHUSE",
    "section": "RWD Working Group",
    "text": "RWD Working Group"
  },
  {
    "objectID": "projects/os_teaching/index.html",
    "href": "projects/os_teaching/index.html",
    "title": "MF9130E Course in R",
    "section": "",
    "text": "MF9130E - Introductory course in statistics\n8-day intensive course on introductory statistics. April 2023 we made it with R rather than propriety software, coupled with live-coding sessions to enhance understanding of basic concepts such as distribution and hypothesis tests.\nRepository\nCourse website\nRead more about the experience in this blogpost."
  },
  {
    "objectID": "projects/sykdomspulsen/index.html#overview",
    "href": "projects/sykdomspulsen/index.html#overview",
    "title": "CSIDS",
    "section": "",
    "text": "CSIDS: the Consortium for Statistics in Disease Surveillance (previously Sykdomspulsen) is a real-time analysis and disease surveillance system designed and developed at the Norwegian Institute of Public Health (NIPH/FHI). It is a unique project that processes new data (e.g. covid-19 cases) shortly after it is available. Complex statistical analyses are automatically run for all locations in Norway, producing reports and alerting various stakeholders. This provides the health authorities the ability to make proactive strategic decisions with the most up-to-date information.\n\n\ncsverse is a number of R packages that have been developed by the Sykdomspulsen team to help with infectious diseases surveillance.\n\n\n\nIn November 2022, the core components of Sykdomspulsen and splverse R packages have been migrated to CSIDS: the Consortium for Statistics in Disease Surveillance. Please refer to CSIDS for ongoing developments.\n\n\n\n\nWe receive data from more than 15 data sources every day\n2 000 000 000+ rows of data and results (1TB)\n1 000+ database tables\n1 000 000+ analyses per day\n1 000+ automatic reports per day\n\n\n\nDownload poster (Norwegian)"
  },
  {
    "objectID": "blog/readnotes_20240218_open_source/index.html",
    "href": "blog/readnotes_20240218_open_source/index.html",
    "title": "Working in Public: The Making and Maintenance of Open Source Software - Nadia Eghbal",
    "section": "",
    "text": "Github as a platform\n\nOn contribution\nNearly half of all contributors only contributed once; which accounted for less than 2% of total commits.\nThe pattern that one or a few developers do most of the work, followed by many casual contributors and even more passive users is the norm, not exception in open source.\nOn casual contributors: they primarily see themselves as users of the project, rather than a part of a contributor community.\nChallenge for maintainers: not how to get more contributors, but how to manage high volume of frequent, low-touch interactions (directing air traffic)\n\nGithub’s open source developers have more in common with solo creators on Twitter, Instagram, YouTube or Twitch.\n\nComparing early internet and social platform nowadays: the early online communities have mailing lists, online forums, membership groups, operated like villages that have their own culture, history and norms. Nowadays creators have much bigger potential audience but the relationship is one-sided, and can be overwhelming.\n\n\nOn free software and hacker\n“Free” means you are able to do what you want with the software, rather than the cost. Libre rather than gratis. At least at the beginning.\nBravado, showmanship, mischievousness, deep mistrust of authority. This culture in the 1980s and 90s was closely linked to the early open source software.\n“Bazaar”: highly participatory, versus “Cathedral”: restricted to a smaller group\nToday’s developer hardly even notice “open source” as a concept anymore, they just want to write and publish their code. They prioritize convenience over freedom or openness.\n\n\nOn licensing\nThe widespread use of permissive licensing is popularized by GH.\nCopyleft licensing (e.g. GNU General Public License GPL) is not commercial friendly as it requires companies to license their software that depend on open source GPL software to have the same license. However GPL gives developers more control over how others use their code in the long run.\n\nAs with any other online content today, sharing is the default.\n\n\n\n\nThe structure of an open source software\n\nOn how projects evolve\nCreate -&gt; Promote and distribute -&gt; Grow\nProjects are promoted like a founder would promote a startup: share on the relevant channels online, give talks at conference and meetups, encourage others to write and talk about it\nA sign that the software is used widely: when the maintainer starts doing more non-code (triage issues, review pull requests) rather than code work.\n\n\nContributor and users\nDepends on technical scope (whether there is much to do), support required (code and admin work), ease of participation (whether on Github) and user adoption (potential contributor base).\nFour types of projects\n\nhigh user growth, high contributor growth: federations. Rare, impactful, the ‘ideal’ of open source project. Roughly 3% of open source projects. Examples: Rust, Node.js, Linux\nhigh user growth, low contributor growth: stadiums: powered by one or a few developers. Centralized.\nlow user growth, high contributor growth: club. Similar to meetup or hobby groups, do not have a wide reach but are loved and built by enthusiasts.\nlow user growth, low contributor growth: toys. Personal project, isn’t trying to grow its user base. Projects on Github with less than 10 stars. Authors do not expect to receive contributions nor do they care about whether people are watching.\n\nDecentralized communities (clubs and federations) have the potential for high user growth - recruit new contributors, reduce contribution friction.\nCentralized communities (stadium) depends on the creators to manage user demand - automation, elimination of noise\n\n\n\nRoles, incentives and relationships\n\nFirms or communities\nFirms (companies, organizations): centralized resources; from a coordination standpoint, managing resources would be more efficient within the same organization - which does not explain why open source developers make software together without formal contracts and financial compensation.\n\n\nThe commons and peer production\nTragedy of the commons: resources depleted by people acting in their own self-interest rather than in the collective interest.\n(One of the 8 design principles by Ostrom on) successful commons:\n\nThose who are affecteed by the rules can participate in modifying them.\n\nStrong sense of group identity maks rules, dispute resolution more meaningful.\nCoordination cost is lower when self-organized based on who wants to do the work most, anyone can do the advertised work and volunteer.\nIn contrast, in companies - solicit, evaluate, hire, manage employees; only employees can do the work limited by their job functions.\nPeople collaborating online for no obvious reason beyond personal satisfaction (intrinsic motivation)\nModular and granular tasks: how tasks are organized, and how big each task is.\nLow coordination costs: quality control over thee modules, integrate the contributions into the finished product\n\n\nContribution beyond code\nSome users do not consider them a contributor, but do actually contribute by education, spreading the word, support (forum), bug reports and more.\nThese active users are similar to contributors but operate independently from project’s contributor community."
  },
  {
    "objectID": "blog/readnotes_20240218_open_source/index.html#notes-from-each-part-1-how-people-make",
    "href": "blog/readnotes_20240218_open_source/index.html#notes-from-each-part-1-how-people-make",
    "title": "Working in Public: The Making and Maintenance of Open Source Software by Nadia Eghbal",
    "section": "Notes from each part 1: How people make",
    "text": "Notes from each part 1: How people make\n\nGithub as a platform\n\n\nThe structure of an open source software\n\n\nRoles, incentives and relationships"
  },
  {
    "objectID": "blog/readnotes_20240218_open_source/index.html#notes-from-part-1-how-people-make",
    "href": "blog/readnotes_20240218_open_source/index.html#notes-from-part-1-how-people-make",
    "title": "Working in Public: The Making and Maintenance of Open Source Software - Nadia Eghbal",
    "section": "",
    "text": "Nearly half of all contributors only contributed once; which accounted for less than 2% of total commits.\nThe pattern that one or a few developers do most of the work, followed by many casual contributors and even more passive users is the norm, not exception in open source.\nOn casual contributors: they primarily see themselves as users of the project, rather than a part of a contributor community.\nChallenge for maintainers: not how to get more contributors, but how to manage high volume of frequent, low-touch interactions (directing air traffic)\n\nGithub’s open source developers have more in common with solo creators on Twitter, Instagram, YouTube or Twitch.\n\nComparing early internet and social platform nowadays: the early online communities have mailing lists, online forums, membership groups, operated like villages that have their own culture, history and norms. Nowadays creators have much bigger potential audience but the relationship is one-sided, and can be overwhelming.\n\n\n\n“Free” means you are able to do what you want with the software, rather than the cost. Libre rather than gratis. At least at the beginning.\nBravado, showmanship, mischievousness, deep mistrust of authority. This culture in the 1980s and 90s was closely linked to the early open source software.\n“Bazaar”: highly participatory, versus “Cathedral”: restricted to a smaller group\nToday’s developer hardly even notice “open source” as a concept anymore, they just want to write and publish their code. They prioritize convenience over freedom or openness.\n\n\n\nThe widespread use of permissive licensing is popularized by GH.\nCopyleft licensing (e.g. GNU General Public License GPL) is not commercial friendly as it requires companies to license their software that depend on open source GPL software to have the same license. However GPL gives developers more control over how others use their code in the long run.\n\nAs with any other online content today, sharing is the default.\n\n\n\n\n\n\n\nCreate -&gt; Promote and distribute -&gt; Grow\nProjects are promoted like a founder would promote a startup: share on the relevant channels online, give talks at conference and meetups, encourage otheers to write and talk about it\nA sign that the software is used widely: when the maintainer starts doing more non-code (triage issues, review pull requests) rathere than code work.\n\n\n\nDepends on **technical *scope (whether there is much to do), support required (code and admin work), ease of participation (whether on Github) and user adoption** (potential contributor base).\nFour types of projects\n\nhigh user growth, high contributor growth: federations. Rare, impactful, the ‘ideal’ of open source project. Roughly 3% of open source projects. Examples: Rust, Node.js, Linux\nhigh user growth, low contributor growth: stadiums: powered by one or a few developers. Centralized.\nlow user growth, high contributor growth: club. Similar to meetup or hobby groups, do not have a wide reach but are loved and built by enthusiasts.\nlow user growth, low contributor growth: toys. Personal project, isn’t trying to grow its user base. Projects on Github with less than 10 stars. Authors do not expect to receive contributions nor do they care about whether people are watching.\n\nDecentralized communities (clubs and federations) have the potential for high user growth - recruit new contributors, reduce contribution friction.\nCentralized communities (stadium) depends on the creators to manage user demand - automation, elimination of noise"
  },
  {
    "objectID": "teaching/med3007/index.html",
    "href": "teaching/med3007/index.html",
    "title": "MED3007",
    "section": "",
    "text": "Genomic data analysis is increasingly important both in medical research and in the clinical practice. In this course, you will learn the basic principles and concepts in the statistical analysis of genomic data. The course will focus on the three main tasks that are frequently involved in genomics: data visualisation, data screening and pre-processing, data modelling (prediction and classification). You will get a practical introduction to RStudio, the best statistical software for analysing genomic data, thus enabling you to perform basic programming tasks, to visualise and analyse genomic data, and to interpret the results."
  },
  {
    "objectID": "teaching/med3007/index.html#course-overview-med3007-at-uio",
    "href": "teaching/med3007/index.html#course-overview-med3007-at-uio",
    "title": "MED3007",
    "section": "",
    "text": "Genomic data analysis is increasingly important both in medical research and in the clinical practice. In this course, you will learn the basic principles and concepts in the statistical analysis of genomic data. The course will focus on the three main tasks that are frequently involved in genomics: data visualisation, data screening and pre-processing, data modelling (prediction and classification). You will get a practical introduction to RStudio, the best statistical software for analysing genomic data, thus enabling you to perform basic programming tasks, to visualise and analyse genomic data, and to interpret the results."
  },
  {
    "objectID": "teaching/med3007/index.html#course-website-in-quarto",
    "href": "teaching/med3007/index.html#course-website-in-quarto",
    "title": "MED3007",
    "section": "Course website in Quarto",
    "text": "Course website in Quarto\nWe developed the course website in Quarto for lecture notes and R lab material. See website here.\nThe associated Github repository is also freely available."
  },
  {
    "objectID": "teaching/teaching.html#statistics",
    "href": "teaching/teaching.html#statistics",
    "title": "Teaching",
    "section": "Statistics",
    "text": "Statistics\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\nERN4110\n\n\nStatistics for master students in nutrition\n\n\n\n\nMED3007\n\n\nStatistical Principles in Genomics\n\n\n\n\nMF9130/MF9130E\n\n\nIntroductory course in statistics\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/technotes_20240506_ohdsi_part1/index.html",
    "href": "blog/technotes_20240506_ohdsi_part1/index.html",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "",
    "text": "The Book of OHDSI written by the OHDSI community.\nWhat is required to go from origin (source data) to destination (evidence):\nOMOP: Observational Medical Outcomes Partnership, aims to identify true drug safety association.\nOMOP CDM: common data model, a mechanism to standardize the structure, content and semantics to make it possible to write statistical code that can be reused at every data site.\nOHDSI community (2014) has created libraries of open-source analytics tools atop OMOP CDM to support:"
  },
  {
    "objectID": "teaching/ern4110/index.html#ern4110-statistikk-for-masterstudenter-i-ernæring",
    "href": "teaching/ern4110/index.html#ern4110-statistikk-for-masterstudenter-i-ernæring",
    "title": "ERN4110",
    "section": "",
    "text": "ERN4110\nEmnet gir en anvendt rettet innføring i statistiske grunnbegreper og sentrale statistiske metoder. Det blir lagt vekt på konkrete eksempler fra ernæringsvitenskap og medisin. PC-øvelser med statistikkpakken Stata er en essensiell del av undervisningen."
  },
  {
    "objectID": "projects/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "href": "projects/nor_mortality/index.html#norwegian-surveillance-system-for-excess-mortality",
    "title": "Mortality Surveillance in Norway",
    "section": "",
    "text": "NorMOMO"
  },
  {
    "objectID": "blog/technotes_20240506_ohdsi_part1/index.html#characterization",
    "href": "blog/technotes_20240506_ohdsi_part1/index.html#characterization",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Characterization",
    "text": "Characterization\n\nWhat happened to the patients.\n\nChapter 11 Characterization\nTypical characterization questions:\n\nHow many patients…?\nHow often does…? What proportion of patients …?\nWhat is the distribution of values for …?\nWhat is the median length of exposure for patients on …?\nOther drugs the patient is using?\n\nDesired output:\n\ncount, percentage\naverages and other descriptive statistics\nprevalence, incidence rate\nrule-based phenotype\ndrug utilization, adherence, treatment pathways, line of therapy\ndisease natural history, co-morbidity profile"
  },
  {
    "objectID": "blog/technotes_20240506_ohdsi_part1/index.html#population-level-estimation",
    "href": "blog/technotes_20240506_ohdsi_part1/index.html#population-level-estimation",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Population-level estimation",
    "text": "Population-level estimation\n\nWhat are the causal effects\n\nChapter 12 Population-level Estimation\nTypical questions:\n\nWhat is the effect of …?\nWhich treatment works better?\nWhat is the risk of X on Y?\nWhat is the time-to-event of …?\n\nDesired output:\n\nRR, HR, OR\nAssociation, correlation\nATE, causal effect"
  },
  {
    "objectID": "blog/technotes_20240506_ohdsi_part1/index.html#patient-level-prediction",
    "href": "blog/technotes_20240506_ohdsi_part1/index.html#patient-level-prediction",
    "title": "Notes: The Book of OHDSI - Data Analytics",
    "section": "Patient-level prediction",
    "text": "Patient-level prediction\n\nWhat will happen to A?\n\nChapter 13 Patient-level Prediction\nTypical questions:\n\nWhat is the chance that this patient will…?\nWho are the candidate for…?\n\nDesired output:\n\nprobability for an individual\nprediction model\nhigh/low risk groups\nprobabilistic phenotype"
  },
  {
    "objectID": "talks/talks.html#upcoming-talks",
    "href": "talks/talks.html#upcoming-talks",
    "title": "Talks",
    "section": "",
    "text": "CAMIS: An Open-Source, Community endeavour for Comparing Analysis Method Implementations\n2024.7.8-11, Salzburg, Austria. Conference link: UseR!"
  },
  {
    "objectID": "blog/readnotes_20240606_bad_pharma/index.html",
    "href": "blog/readnotes_20240606_bad_pharma/index.html",
    "title": "Bad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre",
    "section": "",
    "text": "Industry funded trials were twenty times more likely to give results that are favoring the test drug\n\nOn the need for meta-analysis\n\nPeople would write long review articles surveying the literature - in which they would cite the trial data they come across in a completely unsystematic fashion, often reflecting their own prejudice and values.\n\nOn trials\n\n… mild torture economy: you’re not being paid to do a job, you’re being paid to endure.\n\nOn regulators\n\nfree movement of staff between regulators and drug companies… a fifth of those surveyed said they had been pressured to approve a drug despite reservation about efficacy and safety\n\n\napplication from large companies, which have greater experience with the regulatory process, pass through to approval faster than those from smaller companies\n\nOn surrogate outcomes\n\nthey are approved for showing a benefit on surrogate outcomes, such as blood test, that is only weakly or theoretically associated with the real suffering and death we’re trying to avoid. Sometimes drugs which work well to change surrogate outcomes simply don’t make any difference to the real outcome.\n\nOn comparative effectiveness research\n\nit is a vitally important filed, in many cases the value of finding out what works best among the drugs we already have would hugely exceed the value of developing entirely new ones.\n\nOn safety and efficacy for approved drugs\n\n39 percent patients believe that FDA only approves ‘extremely effective’ drugs, and 25 percent that it only approves drugs without serious side effect. However regulators frequently approve drugs that are only vaguely effective, with serious side effects, on the off-chance that they might be useful to someone, somewhere, when other interventions aren’t an option.\n\nOn drug reviewing\n\nRegulators that have approved a drug are often reluctant to take it off the market, in case it is seen as an admissio of their failure to spot problems in the first place\n\nOn trial patients\n\nthe ‘ideal’ patients are likely to get better, they exaggerate the benefits of drugs, and help expensive new medicines appear to be more cost effective than they really are. ‘External validity’: trial patient being unrepresentative\n\nOn comparison drugs\n\nit is common to see trials where a new drug is compared to a competitor that is known to be useless; or with a good competitor at a stupidly low (or high) dose\n\nOn random variation in the data\n\nearly stopping because you peeked in the results. should set up stopping rules, specified before the trial begins\nneed a large trial to detect a small difference between two treatments, and a very large trial to be confident that two drugs are equally effective\n(multiple testing, sub-group analysis): measuring lots of things, some will be statistically significant, simply from the natural random variation in all trial data.\n\nOn presenting the results\n\npercent reduction in the risk of heart attack (risk difference)\nrelative risk reduction\npresenting the results as relative risk reduction overstates the benefits\n\nWays trials go wrong\n\nunrepresentative patients\ntoo brief\nmeasure the wrong outcomes\ngo missing, if the result is unflattering\nanalysed wrongly\n\nOn ‘simple trial’ using EHR\n\nat present trials are very expensive. Many struggle to recruit enough patients, many struggle to recruit everyday doctors who don’t want to get involved in the mess of filing out patient report forms, calling patients back for extra appointments, doing extra measurements and so on\nsimple trials have disadvantage of being not blinded - patients know what drug they’ve received\npragmatic trials are cheap\nthese trials run forever and follow-up data are easy to get\n\nOn marketing\n\nSome have estimated that the pharmaceutical industry overall spends twice as much on marketing and promotion as it does on research and developments\n\n\nDrugs are advertised more when the number of potential patients, rather than the current patients, is large."
  },
  {
    "objectID": "blog/readnotes_20240606_bad_pharma/index.html#notes-from-the-book",
    "href": "blog/readnotes_20240606_bad_pharma/index.html#notes-from-the-book",
    "title": "Bad Pharma: How medicine is broken, and how we can fix it - Ben Goldacre",
    "section": "",
    "text": "Industry funded trials were twenty times more likely to give results that are favoring the test drug\n\nOn the need for meta-analysis\n\nPeople would write long review articles surveying the literature - in which they would cite the trial data they come across in a completely unsystematic fashion, often reflecting their own prejudice and values.\n\nOn trials\n\n… mild torture economy: you’re not being paid to do a job, you’re being paid to endure.\n\nOn regulators\n\nfree movement of staff between regulators and drug companies… a fifth of those surveyed said they had been pressured to approve a drug despite reservation about efficacy and safety\n\n\napplication from large companies, which have greater experience with the regulatory process, pass through to approval faster than those from smaller companies\n\nOn surrogate outcomes\n\nthey are approved for showing a benefit on surrogate outcomes, such as blood test, that is only weakly or theoretically associated with the real suffering and death we’re trying to avoid. Sometimes drugs which work well to change surrogate outcomes simply don’t make any difference to the real outcome.\n\nOn comparative effectiveness research\n\nit is a vitally important filed, in many cases the value of finding out what works best among the drugs we already have would hugely exceed the value of developing entirely new ones.\n\nOn safety and efficacy for approved drugs\n\n39 percent patients believe that FDA only approves ‘extremely effective’ drugs, and 25 percent that it only approves drugs without serious side effect. However regulators frequently approve drugs that are only vaguely effective, with serious side effects, on the off-chance that they might be useful to someone, somewhere, when other interventions aren’t an option.\n\nOn drug reviewing\n\nRegulators that have approved a drug are often reluctant to take it off the market, in case it is seen as an admissio of their failure to spot problems in the first place\n\nOn trial patients\n\nthe ‘ideal’ patients are likely to get better, they exaggerate the benefits of drugs, and help expensive new medicines appear to be more cost effective than they really are. ‘External validity’: trial patient being unrepresentative\n\nOn comparison drugs\n\nit is common to see trials where a new drug is compared to a competitor that is known to be useless; or with a good competitor at a stupidly low (or high) dose\n\nOn random variation in the data\n\nearly stopping because you peeked in the results. should set up stopping rules, specified before the trial begins\nneed a large trial to detect a small difference between two treatments, and a very large trial to be confident that two drugs are equally effective\n(multiple testing, sub-group analysis): measuring lots of things, some will be statistically significant, simply from the natural random variation in all trial data.\n\nOn presenting the results\n\npercent reduction in the risk of heart attack (risk difference)\nrelative risk reduction\npresenting the results as relative risk reduction overstates the benefits\n\nWays trials go wrong\n\nunrepresentative patients\ntoo brief\nmeasure the wrong outcomes\ngo missing, if the result is unflattering\nanalysed wrongly\n\nOn ‘simple trial’ using EHR\n\nat present trials are very expensive. Many struggle to recruit enough patients, many struggle to recruit everyday doctors who don’t want to get involved in the mess of filing out patient report forms, calling patients back for extra appointments, doing extra measurements and so on\nsimple trials have disadvantage of being not blinded - patients know what drug they’ve received\npragmatic trials are cheap\nthese trials run forever and follow-up data are easy to get\n\nOn marketing\n\nSome have estimated that the pharmaceutical industry overall spends twice as much on marketing and promotion as it does on research and developments\n\n\nDrugs are advertised more when the number of potential patients, rather than the current patients, is large."
  },
  {
    "objectID": "misc/literature.html",
    "href": "misc/literature.html",
    "title": "Hello, I'm Chi",
    "section": "",
    "text": "antibiotics stewardship\ninfections (community, hospital)\nelectronic health records\nquality assurance: prescripton and use"
  },
  {
    "objectID": "misc/literature.html#ab-x-ehr",
    "href": "misc/literature.html#ab-x-ehr",
    "title": "Hello, I'm Chi",
    "section": "AB x EHR",
    "text": "AB x EHR\nMoehring 2021, EHR identify AB use among hospitalized patients\nReenggli 2021: assess EHR conversion into AB stewardship indicators\nCairns 2021: integrate AB stewardship with EHR in australia\nJenkins 2022: AB stewardship using electornic prescribing systems, review of intervention and outcome measures\nKuijpers 2024, excessive length of AB duration for HAI, support AB stewardship high relevance"
  },
  {
    "objectID": "misc/literature.html#ehr-doctor-experience-system-design",
    "href": "misc/literature.html#ehr-doctor-experience-system-design",
    "title": "Hello, I'm Chi",
    "section": "EHR doctor experience, system design",
    "text": "EHR doctor experience, system design\nOstrer 2023, real time benefit tools must be designed to serve both clinicians and patients. mentions burden\nKawamoto 2019 Association of EHR add-on app for neonatal bilirubin management, physician efficiency and care quality. Good system and add-on can save clinician time and improve patient care.\nTsai 2020, EHR implementation, barriers to adoption and use high relevance\nOverhage 2020, time spent using EHR\n\nburnout\nTajirian 2020, influence of EHR on physician burnout high relevance\nKhairat 2020, EHR use with physician fatigue and efficiency\nKroth 2019, factors of EHR design and use for physician stress and burnout high relevance\nMelnick 2020, EHR usability, task load and burnout high relevance\nHilliard 2020, factors associated with burnout\nMore on this topic\nhttps://www.sciencedirect.com/science/article/abs/pii/S0897189718301356\nhttps://journals.sagepub.com/doi/full/10.1177/20552076231220241\nhttps://jamanetwork.com/journals/jamanetworkopen/article-abstract/2778909\n\n\nDesign\nGascon 2013 the process of designing a EHR lab request module\nTorres 2017, effect of EHR design on documentation and compliance high relevance"
  },
  {
    "objectID": "misc/literature.html#ehr-data-quality-use-for-prediction-model-development",
    "href": "misc/literature.html#ehr-data-quality-use-for-prediction-model-development",
    "title": "Hello, I'm Chi",
    "section": "EHR data quality, use for prediction model development",
    "text": "EHR data quality, use for prediction model development\n\nerror\nbell 2020\nhttps://www.sciencedirect.com/science/article/abs/pii/S2213076420300439\nLi 2024, comparison of error reporting systems. 82% undetected, 90% had no corresponding incident report, some errors not reported at all\nWestbrook 2020, changes in medication administration error rates, associated with EHR\nGildon 2019, impact of EHR on prescribing errors in pediatric clinics. introduction is relevant for pointing out the need for well designed system\nGinzburg 2018, use clinical decision support within EHR to reduce incorrect prescribing for acute sinusitis. shows prompt for physicians to fill in reason for why AB is required high relevance\n\n\nbias\nGianfrancesco 2018, potential biases in ML algorithms using EHR data. Existing EHR disparities should not be amplified by thoughtless or excess reliance on machines.\nAgniel 2018, biases in EHR due to process within the healthcare system. healthcare processes must be addressed in the analysis of observational health data, context is important. high relevance\nBower, biases in EHR based surveillance of cardiovascular disease risk\nHarding2024, addressing common sources of bias in type 2 diabetes following covid, using EHR\nBica 2020, current and future methods to address underlying challenges from EHR to treatment effects mid relevance to manuscript, but important to myself\nDesai 2020, prediction using admin EHR, hear failure"
  },
  {
    "objectID": "misc/literature.html#ab-stewardship-not-so-relevant",
    "href": "misc/literature.html#ab-stewardship-not-so-relevant",
    "title": "Hello, I'm Chi",
    "section": "AB stewardship (not so relevant)",
    "text": "AB stewardship (not so relevant)\nRenggli 2021: consumption of anti-meticillin resistant staphylococcus aureus antibiotics in Swiss hospitals is associateed with antibiotic stewardship measures\nVaughn 2021: AB overuse and stewardship at hospital discharge\nGraber 2019: electronic tools to decrease AB use"
  },
  {
    "objectID": "talks/rstats_20240613_teaching/index.html",
    "href": "talks/rstats_20240613_teaching/index.html",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "",
    "text": "Time and place: June 13 2024. Online\nSlides for this talk can be accessed here."
  },
  {
    "objectID": "talks/rstats_20240613_teaching/index.html#about-the-topic",
    "href": "talks/rstats_20240613_teaching/index.html#about-the-topic",
    "title": "A one year recap on teaching statistcis to medical students: how can R and Quarto help?",
    "section": "About the topic",
    "text": "About the topic\nThe Department of Biostatistics at University of Oslo offer statistics courses at different levels for medical students and PhD candidates with clinical backgrounds. The courses were traditionally taught with a focus on theory instead of data analysis, where SPSS and STATA were the tools of choice.\nSince 2023 spring semester, we have been gradually transforming some of our statistics courses into R, using Quarto course websites and Carpentries style live-coding instruction. With new Quarto tools (such as WebR) we also added interactivity in the code blocks. So far we have transformed two courses with over 100 students who have almost no programming experience. We have observed impressive progress in the skill development, and received significantly more positive feedback when it comes to statistics education.\nIn this talk, I would like to share our experience on the successes and challenges throughout the process. Looking back, is it cost-effective? Definitely. Can we do better in the future? Almost surely. If you are also planning to adopt new technology in your teaching activities, join us to learn more about what you can do to make the transition happen!\nCourse website can be accessed here"
  }
]